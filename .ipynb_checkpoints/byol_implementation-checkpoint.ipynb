{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067a895d-6081-4114-a848-2b6f88b8713c",
   "metadata": {},
   "source": [
    "# BYOL Implementation for LOTSS DR2 Radio Galaxies\n",
    "\n",
    "**Project G: Interpretable Latent Space Semantics for Radio Galaxies**\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook creates an interpretable latent space for radio galaxy morphologies using a modified BYOL (Bootstrap Your Own Latent) architecture trained from scratch. The goal is to produce semantically meaningful embeddings where morphological properties vary smoothly, enabling interpolation and exploration of the latent space.\n",
    "\n",
    "## Method\n",
    "\n",
    "We adapt BYOL's contrastive learning approach using morphological classifications from Horton et al. (2025, A&A, 699, A338):\n",
    "\n",
    "**Standard BYOL**: Positive pairs = two augmentations of the same image  \n",
    "**Our modification**: Positive pairs = different images with the same morphological label\n",
    "\n",
    "This combines self-supervised learning with weak supervision to create a latent space where:\n",
    "- Similar morphologies cluster together (FRI, FRII, Hybrid, etc.)\n",
    "- Latent dimensions correspond to interpretable features\n",
    "- Smooth transitions exist between morphological classes\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- **Backbone**: EfficientNet-B0 (trained from scratch, no ImageNet pretraining)\n",
    "- **Projection head**: 1280 → 4096 → 256 dimensions\n",
    "- **Predictor head**: 256 → 4096 → 256 dimensions\n",
    "- **Input**: 89×89 greyscale numpy arrays (LOTSS DR2 cutouts)\n",
    "- **Output**: 256-dimensional embeddings\n",
    "- **Target network**: EMA updates with τ = 0.99\n",
    "\n",
    "## Data\n",
    "\n",
    "- Source: LoTSS DR2 (Shimwell et al. 2022)\n",
    "- Labels: Horton et al. (2025) morphological classifications\n",
    "  - 9,985 visually classified sources\n",
    "  - Classes: FRI (2406), FRII (4693), Hybrid (751), Relaxed doubles (361), etc.\n",
    "- Image size: 89×89 pixels (radio continuum at 144 MHz)\n",
    "\n",
    "## References\n",
    "\n",
    "- BYOL: Grill et al. (2020), NeurIPS 33\n",
    "- Morphology labels: Horton et al. (2025), A&A, 699, A338\n",
    "- LoTSS DR2: Shimwell et al. (2022), A&A, 659, A1\n",
    "\n",
    "(Text generated with Claude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4189f1-7496-431d-9ded-97f862bec063",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cead9fa-efd0-4720-9af2-648abb911715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed)\n",
    "# !pip install torch torchvision astropy pandas numpy matplotlib tqdm --break-system-packages\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# Astronomical data\n",
    "from astropy.io import fits\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319347f2-2091-4114-846c-9552b5b72a46",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load LOTSS DR2 radio galaxy images (89×89 numpy arrays) and multi-label morphological classifications from Horton et al. (2025).\n",
    "\n",
    "### Data Structure:\n",
    "- **Images**: Already split into train/val/test sets\n",
    "  - `train_images.npy`, `val_images.npy`, `test_images.npy`\n",
    "  - Each: shape `(N, 89, 89)` - greyscale radio continuum\n",
    "  \n",
    "- **Labels**: Multi-label classification scheme, shape `(N, 4)`\n",
    "  - `labels[i] = [initial_class, morphology, environment, derived]`\n",
    "  - Position 0: **Initial classification** (FRI, FRII, Hybrid, Spiral, Relaxed)\n",
    "  - Position 1: **Morphology** (C-curve, S-curve, Wings, X-shaped, etc.)\n",
    "  - Position 2: **Environment** (Cluster, Merger, Diffuse, Unknown)\n",
    "  - Position 3: **Derived catalogue** (Hybrid FRI/FRII, Curved FRIs, etc.)\n",
    "  - Value `0` = Not Assigned (N/A)\n",
    "\n",
    "### Data Source:\n",
    "- LoTSS DR2 cutouts (Shimwell et al. 2022)\n",
    "- Classifications (Horton et al. 2025, A&A, 699, A338, Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aef7f2-45ac-4898-be1c-fc75f47abf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LABEL NAME MAPPINGS (from Horton et al. 2025, Table 1)\n",
    "# =============================================================================\n",
    "\n",
    "# Position 0: Initial classification\n",
    "INITIAL_CLASS_NAMES = {\n",
    "    0: 'N/A',\n",
    "    1: 'FRI',\n",
    "    2: 'FRII',\n",
    "    3: 'Hybrid',\n",
    "    4: 'Spiral',\n",
    "    5: 'Relaxed double'\n",
    "}\n",
    "\n",
    "# Position 1: Morphology features\n",
    "MORPHOLOGY_NAMES = {\n",
    "    0: 'N/A',\n",
    "    1: 'C-curvature',\n",
    "    2: 'S-curvature',\n",
    "    3: 'Misalignment',\n",
    "    4: 'Wings',\n",
    "    5: 'X-shaped',\n",
    "    6: 'Straight jets',\n",
    "    7: 'Multiple hotspots',\n",
    "    8: 'Continuous jets',\n",
    "    9: 'Banding',\n",
    "    10: 'One-sided',\n",
    "    11: 'Restarted'\n",
    "}\n",
    "\n",
    "# Position 2: Environment\n",
    "ENVIRONMENT_NAMES = {\n",
    "    0: 'N/A',\n",
    "    1: 'Cluster',\n",
    "    2: 'Merger',\n",
    "    3: 'Diffuse emission',\n",
    "    4: 'Unknown'\n",
    "}\n",
    "\n",
    "# Position 3: Derived catalogue\n",
    "DERIVED_NAMES = {\n",
    "    0: 'N/A',\n",
    "    1: 'Compact sources & other hybrids',\n",
    "    2: 'Hybrid FRI/FRII',\n",
    "    3: 'Curved FRIs',\n",
    "    4: 'Curved FRIIs',\n",
    "    5: 'Straight & multi hotspots'\n",
    "}\n",
    "\n",
    "# Combined mapping for easy access\n",
    "LABEL_SCHEMES = {\n",
    "    'initial': INITIAL_CLASS_NAMES,\n",
    "    'morphology': MORPHOLOGY_NAMES,\n",
    "    'environment': ENVIRONMENT_NAMES,\n",
    "    'derived': DERIVED_NAMES\n",
    "}\n",
    "\n",
    "print(\"✓ Label mappings configured\")\n",
    "print(f\"  Initial classes:   {len(INITIAL_CLASS_NAMES)-1} (+ N/A)\")\n",
    "print(f\"  Morphology types:  {len(MORPHOLOGY_NAMES)-1} (+ N/A)\")\n",
    "print(f\"  Environment types: {len(ENVIRONMENT_NAMES)-1} (+ N/A)\")\n",
    "print(f\"  Derived types:     {len(DERIVED_NAMES)-1} (+ N/A)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba56c87-6db1-4c48-9eb2-5775a6975260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# UPDATE THESE PATHS to your data locations\n",
    "DATA_DIR = Path('./data')\n",
    "\n",
    "# Separate train/val/test files\n",
    "TRAIN_IMAGES_PATH = DATA_DIR / 'train_images.npy'\n",
    "TRAIN_LABELS_PATH = DATA_DIR / 'train_labels.npy'\n",
    "\n",
    "VAL_IMAGES_PATH = DATA_DIR / 'val_images.npy'\n",
    "VAL_LABELS_PATH = DATA_DIR / 'val_labels.npy'\n",
    "\n",
    "TEST_IMAGES_PATH = DATA_DIR / 'test_images.npy'\n",
    "TEST_LABELS_PATH = DATA_DIR / 'test_labels.npy'\n",
    "\n",
    "# Optional: catalogue with source names and metadata\n",
    "CATALOGUE_PATH = DATA_DIR / 'horton2025_catalogue.csv'\n",
    "\n",
    "print(\"Loading LOTSS DR2 data (train/val/test split)...\\n\")\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training set...\")\n",
    "train_images = np.load(TRAIN_IMAGES_PATH)\n",
    "train_labels = np.load(TRAIN_LABELS_PATH)\n",
    "print(f\"  ✓ Train: {train_images.shape[0]} samples\")\n",
    "\n",
    "# Load validation data\n",
    "print(\"Loading validation set...\")\n",
    "val_images = np.load(VAL_IMAGES_PATH)\n",
    "val_labels = np.load(VAL_LABELS_PATH)\n",
    "print(f\"  ✓ Val:   {val_images.shape[0]} samples\")\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test set...\")\n",
    "test_images = np.load(TEST_IMAGES_PATH)\n",
    "test_labels = np.load(TEST_LABELS_PATH)\n",
    "print(f\"  ✓ Test:  {test_images.shape[0]} samples\")\n",
    "\n",
    "# Load catalogue (optional)\n",
    "catalogue = None\n",
    "if CATALOGUE_PATH.exists():\n",
    "    catalogue = pd.read_csv(CATALOGUE_PATH)\n",
    "    print(f\"  ✓ Catalogue: {len(catalogue)} sources\\n\")\n",
    "\n",
    "# Combine for summary statistics\n",
    "all_images = np.concatenate([train_images, val_images, test_images])\n",
    "all_labels = np.concatenate([train_labels, val_labels, test_labels])\n",
    "\n",
    "print(f\"✓ Data loaded successfully\")\n",
    "print(f\"  Total samples: {len(all_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc90884-92fe-407c-901c-367093413b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def validate_data(images, labels, split_name):\n",
    "    \"\"\"Validate image and label dimensions.\"\"\"\n",
    "    \n",
    "    # Check image shape\n",
    "    if images.ndim == 4 and images.shape[-1] == 1:\n",
    "        images = images.squeeze(-1)\n",
    "        print(f\"  ⚠ {split_name}: Squeezed channel dim (N,89,89,1) → (N,89,89)\")\n",
    "    \n",
    "    assert images.ndim == 3, f\"{split_name}: Expected 3D array (N,H,W), got {images.shape}\"\n",
    "    assert images.shape[1] == images.shape[2] == 89, \\\n",
    "        f\"{split_name}: Expected 89×89, got {images.shape[1]}×{images.shape[2]}\"\n",
    "    \n",
    "    # Check label shape\n",
    "    assert labels.ndim == 2, f\"{split_name}: Expected 2D labels (N,4), got {labels.shape}\"\n",
    "    assert labels.shape[1] == 4, \\\n",
    "        f\"{split_name}: Expected 4 label columns, got {labels.shape[1]}\"\n",
    "    assert len(images) == len(labels), \\\n",
    "        f\"{split_name}: Mismatch - {len(images)} images but {len(labels)} labels\"\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Validate each split\n",
    "print(\"\\nValidating data...\")\n",
    "train_images, train_labels = validate_data(train_images, train_labels, \"Train\")\n",
    "val_images, val_labels = validate_data(val_images, val_labels, \"Val\")\n",
    "test_images, test_labels = validate_data(test_images, test_labels, \"Test\")\n",
    "print(\"✓ All splits validated\\n\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"DATA SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Train images:        {train_images.shape}\")\n",
    "print(f\"Train labels:        {train_labels.shape}\")\n",
    "print(f\"Val images:          {val_images.shape}\")\n",
    "print(f\"Val labels:          {val_labels.shape}\")\n",
    "print(f\"Test images:         {test_images.shape}\")\n",
    "print(f\"Test labels:         {test_labels.shape}\")\n",
    "print(f\"\\nImage dtype:         {all_images.dtype}\")\n",
    "print(f\"Label dtype:         {all_labels.dtype}\")\n",
    "print(f\"Image range:         [{all_images.min():.4f}, {all_images.max():.4f}]\")\n",
    "print(f\"Image mean±std:      {all_images.mean():.4f} ± {all_images.std():.4f}\")\n",
    "print(f\"Total memory:        {all_images.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "n_nan = np.isnan(all_images).sum()\n",
    "n_inf = np.isinf(all_images).sum()\n",
    "if n_nan > 0 or n_inf > 0:\n",
    "    print(f\"⚠ WARNING: {n_nan} NaN, {n_inf} Inf values\")\n",
    "else:\n",
    "    print(f\"✓ No NaN/Inf values detected\")\n",
    "\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f407b1-40af-4b11-b6dd-8db0b0bad877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLASS DISTRIBUTION (Multi-Label)\n",
    "# =============================================================================\n",
    "\n",
    "def print_class_distribution(labels, split_name, scheme_names):\n",
    "    \"\"\"Print distribution for each classification scheme.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{split_name} - CLASS DISTRIBUTION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    scheme_titles = ['Initial Classification', 'Morphology', 'Environment', 'Derived']\n",
    "    \n",
    "    for col_idx, (scheme_key, title) in enumerate(zip(scheme_names.keys(), scheme_titles)):\n",
    "        print(f\"\\n{title} (column {col_idx}):\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        # Get labels for this column\n",
    "        col_labels = labels[:, col_idx]\n",
    "        unique, counts = np.unique(col_labels, return_counts=True)\n",
    "        \n",
    "        # Sort by count (descending), but put N/A first\n",
    "        sorted_indices = np.argsort(-counts)\n",
    "        if 0 in unique:  # Move N/A to front\n",
    "            na_idx = np.where(unique == 0)[0][0]\n",
    "            sorted_indices = np.concatenate([[na_idx], \n",
    "                                            sorted_indices[sorted_indices != na_idx]])\n",
    "        \n",
    "        for idx in sorted_indices:\n",
    "            label_id = unique[idx]\n",
    "            count = counts[idx]\n",
    "            pct = 100 * count / len(labels)\n",
    "            name = scheme_names[scheme_key].get(label_id, f'Unknown({label_id})')\n",
    "            \n",
    "            # Highlight N/A\n",
    "            marker = \"  \" if label_id != 0 else \"→ \"\n",
    "            print(f\"{marker}{name:30s} (id={label_id:2d}): {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Print distributions for all splits\n",
    "print_class_distribution(train_labels, \"TRAIN SET\", LABEL_SCHEMES)\n",
    "print_class_distribution(val_labels, \"VALIDATION SET\", LABEL_SCHEMES)\n",
    "print_class_distribution(test_labels, \"TEST SET\", LABEL_SCHEMES)\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25560fb-84b4-4888-8886-d09cfaaa3d53",
   "metadata": {},
   "source": [
    "## BYOL Architecture (From Scratch)\n",
    "\n",
    "We implement BYOL without any pretrained weights or external libraries.\n",
    "\n",
    "### Components:\n",
    "1. **Backbone**: EfficientNet-B0 (1 channel input, 1280-dim output)\n",
    "2. **Projection MLP**: 1280 → 4096 → 256\n",
    "3. **Predictor MLP**: 256 → 4096 → 256\n",
    "4. **Target network**: EMA copy of encoder + projector (τ=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0f4ab-7861-4b41-a619-c1dadf753eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientnet_b0(num_channels=1, img_size=89):\n",
    "    \"\"\"\n",
    "    Create EfficientNet-B0 from scratch (no pretraining).\n",
    "    Modified for single-channel input (radio continuum).\n",
    "    \n",
    "    Args:\n",
    "        num_channels: Number of input channels (1 for greyscale radio)\n",
    "        img_size: Input image size (89 for LOTSS DR2 cutouts)\n",
    "    \n",
    "    Returns:\n",
    "        model: EfficientNet-B0 backbone with modified input and no classifier\n",
    "    \"\"\"\n",
    "    # Load model architecture WITHOUT ImageNet weights\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    \n",
    "    # Modify first conv layer for single-channel input\n",
    "    original_conv = model.features[0][0]\n",
    "    model.features[0][0] = nn.Conv2d(\n",
    "        num_channels,  # 1 channel instead of 3 (RGB)\n",
    "        original_conv.out_channels,\n",
    "        kernel_size=original_conv.kernel_size,\n",
    "        stride=original_conv.stride,\n",
    "        padding=original_conv.padding,\n",
    "        bias=False\n",
    "    )\n",
    "    \n",
    "    # Initialize the new conv layer (since we changed it)\n",
    "    nn.init.kaiming_normal_(model.features[0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    # Remove classification head (keep feature extractor only)\n",
    "    model.classifier = nn.Identity()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create and test backbone\n",
    "print(\"Creating EfficientNet-B0 backbone...\")\n",
    "backbone = create_efficientnet_b0(num_channels=1, img_size=89).to(device)\n",
    "\n",
    "# Test output dimension with 89x89 input\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(2, 1, 89, 89).to(device)\n",
    "    backbone_output = backbone(dummy_input)\n",
    "    backbone_dim = backbone_output.shape[1]\n",
    "\n",
    "print(f\"\\n✓ Backbone created successfully\")\n",
    "print(f\"  Architecture: EfficientNet-B0 (from scratch, no pretraining)\")\n",
    "print(f\"  Input shape: {dummy_input.shape}\")\n",
    "print(f\"  Output shape: {backbone_output.shape}\")\n",
    "print(f\"  Feature dimension: {backbone_dim}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in backbone.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76245ecd-71ab-482d-84f7-bc4b14785d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for projection and prediction heads.\n",
    "    Architecture: input_dim → hidden_dim (BN + ReLU) → output_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Test MLP dimensions\n",
    "print(\"Testing MLP architectures...\")\n",
    "\n",
    "# Projector: 1280 → 4096 → 256\n",
    "projector = MLP(input_dim=1280, hidden_dim=4096, output_dim=256)\n",
    "with torch.no_grad():\n",
    "    proj_test = projector(torch.randn(2, 1280))\n",
    "print(f\"✓ Projector: {1280} → {4096} → {256}, output shape: {proj_test.shape}\")\n",
    "\n",
    "# Predictor: 256 → 4096 → 256\n",
    "predictor = MLP(input_dim=256, hidden_dim=4096, output_dim=256)\n",
    "with torch.no_grad():\n",
    "    pred_test = predictor(torch.randn(2, 256))\n",
    "print(f\"✓ Predictor: {256} → {4096} → {256}, output shape: {pred_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53930b3-26d5-4eda-bf20-00a74827bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    \"\"\"\n",
    "    Bootstrap Your Own Latent (BYOL)\n",
    "    \n",
    "    Two networks:\n",
    "    - Online network (trainable): encoder → projector → predictor\n",
    "    - Target network (EMA, frozen): encoder → projector\n",
    "    \n",
    "    The online network learns to predict the target network's representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, projection_dim=256, hidden_dim=4096, img_size=89):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Get backbone output dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, img_size, img_size)\n",
    "            if next(backbone.parameters()).is_cuda:\n",
    "                dummy = dummy.cuda()\n",
    "            backbone_dim = backbone(dummy).shape[1]\n",
    "        \n",
    "        print(f\"Backbone output dimension: {backbone_dim}\")\n",
    "        \n",
    "        # === ONLINE NETWORK (trainable) ===\n",
    "        self.online_encoder = backbone\n",
    "        self.online_projector = MLP(backbone_dim, hidden_dim, projection_dim)\n",
    "        self.predictor = MLP(projection_dim, hidden_dim, projection_dim)\n",
    "        \n",
    "        # === TARGET NETWORK (frozen, updated via EMA) ===\n",
    "        self.target_encoder = copy.deepcopy(backbone)\n",
    "        self.target_projector = copy.deepcopy(self.online_projector)\n",
    "        \n",
    "        # Freeze target network parameters\n",
    "        for param in self.target_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.target_projector.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass for two views.\n",
    "        \n",
    "        Args:\n",
    "            x1: First view (batch_size, 1, 89, 89)\n",
    "            x2: Second view (batch_size, 1, 89, 89)\n",
    "        \n",
    "        Returns:\n",
    "            p1, p2: Predictions from online network\n",
    "            t1, t2: Targets from target network (detached)\n",
    "        \"\"\"\n",
    "        # === ONLINE NETWORK ===\n",
    "        # Encode both views\n",
    "        z1_online = self.online_encoder(x1)  # (B, 1280)\n",
    "        z2_online = self.online_encoder(x2)  # (B, 1280)\n",
    "        \n",
    "        # Project to lower dimension\n",
    "        proj1_online = self.online_projector(z1_online)  # (B, 256)\n",
    "        proj2_online = self.online_projector(z2_online)  # (B, 256)\n",
    "        \n",
    "        # Predict target representations\n",
    "        p1 = self.predictor(proj1_online)  # (B, 256)\n",
    "        p2 = self.predictor(proj2_online)  # (B, 256)\n",
    "        \n",
    "        # === TARGET NETWORK (no gradients) ===\n",
    "        with torch.no_grad():\n",
    "            z1_target = self.target_encoder(x1)  # (B, 1280)\n",
    "            z2_target = self.target_encoder(x2)  # (B, 1280)\n",
    "            \n",
    "            t1 = self.target_projector(z1_target)  # (B, 256)\n",
    "            t2 = self.target_projector(z2_target)  # (B, 256)\n",
    "        \n",
    "        return p1, p2, t1, t2\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_target_network(self, momentum=0.99):\n",
    "        \"\"\"\n",
    "        Update target network using exponential moving average (EMA).\n",
    "        \n",
    "        θ_target = τ * θ_target + (1 - τ) * θ_online\n",
    "        \n",
    "        Args:\n",
    "            momentum: EMA decay rate (τ). Default: 0.99\n",
    "        \"\"\"\n",
    "        # Update target encoder\n",
    "        for online_param, target_param in zip(\n",
    "            self.online_encoder.parameters(), \n",
    "            self.target_encoder.parameters()\n",
    "        ):\n",
    "            target_param.data = momentum * target_param.data + (1 - momentum) * online_param.data\n",
    "        \n",
    "        # Update target projector\n",
    "        for online_param, target_param in zip(\n",
    "            self.online_projector.parameters(), \n",
    "            self.target_projector.parameters()\n",
    "        ):\n",
    "            target_param.data = momentum * target_param.data + (1 - momentum) * online_param.data\n",
    "\n",
    "\n",
    "# Test BYOL model\n",
    "print(\"\\nTesting BYOL model...\")\n",
    "test_model = BYOL(backbone, projection_dim=256, hidden_dim=4096, img_size=89)\n",
    "with torch.no_grad():\n",
    "    x1_test = torch.randn(2, 1, 89, 89)\n",
    "    x2_test = torch.randn(2, 1, 89, 89)\n",
    "    p1, p2, t1, t2 = test_model(x1_test, x2_test)\n",
    "\n",
    "print(f\"✓ BYOL model created\")\n",
    "print(f\"  Predictions (p1, p2): {p1.shape}, {p2.shape}\")\n",
    "print(f\"  Targets (t1, t2): {t1.shape}, {t2.shape}\")\n",
    "print(f\"  Target parameters frozen: {not next(test_model.target_encoder.parameters()).requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616ebac-dfcd-4866-9f9a-4ffcd1592678",
   "metadata": {},
   "source": [
    "### Loss function for BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d871d-e556-4043-a0ae-84f0fb0b8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byol_loss(p1, p2, t1, t2):\n",
    "    \"\"\"\n",
    "    BYOL loss function (symmetrised mean squared error on unit hypersphere).\n",
    "    \n",
    "    Loss = MSE(normalize(p1), normalize(t2)) + MSE(normalize(p2), normalize(t1))\n",
    "    \n",
    "    Equivalently (using cosine similarity):\n",
    "    Loss = 2 - 2 * [cos_sim(p1, t2) + cos_sim(p2, t1)]\n",
    "    \n",
    "    Args:\n",
    "        p1, p2: Predictions from online network (B, 256)\n",
    "        t1, t2: Targets from target network (B, 256)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss value (lower is better)\n",
    "    \"\"\"\n",
    "    # Normalize predictions and targets to unit hypersphere (L2 norm = 1)\n",
    "    p1 = F.normalize(p1, dim=-1, p=2)\n",
    "    p2 = F.normalize(p2, dim=-1, p=2)\n",
    "    t1 = F.normalize(t1, dim=-1, p=2)\n",
    "    t2 = F.normalize(t2, dim=-1, p=2)\n",
    "    \n",
    "    # Compute loss using cosine similarity\n",
    "    # We want high cosine similarity, so we minimize (2 - 2*cosine_similarity)\n",
    "    loss = 2 - 2 * (p1 * t2).sum(dim=-1).mean() - 2 * (p2 * t1).sum(dim=-1).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test loss function\n",
    "print(\"Testing BYOL loss...\")\n",
    "with torch.no_grad():\n",
    "    # Perfect predictions (should give loss ≈ 0)\n",
    "    loss_perfect = byol_loss(t1, t2, t1, t2)\n",
    "    print(f\"✓ Loss (perfect match): {loss_perfect.item():.6f} (should be ≈0)\")\n",
    "    \n",
    "    # Random predictions (should give loss ≈ 2)\n",
    "    random_p1 = torch.randn(2, 256)\n",
    "    random_p2 = torch.randn(2, 256)\n",
    "    loss_random = byol_loss(random_p1, random_p2, t1, t2)\n",
    "    print(f\"✓ Loss (random): {loss_random.item():.6f} (should be ≈2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f8109-99bf-4f33-87a2-348c47a9f728",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "- **Batch size**: 32 (reduce to 16 if GPU memory issues)\n",
    "- **Epochs**: 100\n",
    "- **Learning rate**: 5e-4\n",
    "- **Optimizer**: Adam\n",
    "- **EMA momentum (τ)**: 0.99 (target network update rate)\n",
    "\n",
    "### Data Strategy\n",
    "- **Train/Val split**: 80/20 (stratified by class)\n",
    "- **Positive pairs**: Different images with same morphological label\n",
    "- **Augmentations**: Crops, flips, rotations, Gaussian blur (no colour jitter)\n",
    "\n",
    "### Training Utilities\n",
    "- Checkpoint saving every 10 epochs\n",
    "- Loss logging to CSV\n",
    "- Optional: Cosine annealing LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb27482-f5b4-42e5-83d3-83458686c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 32  # Reduce to 16 if OOM errors\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "EMA_MOMENTUM = 0.99  # τ for target network EMA\n",
    "\n",
    "# Model config\n",
    "PROJECTION_DIM = 256\n",
    "HIDDEN_DIM = 4096\n",
    "IMG_SIZE = 89\n",
    "\n",
    "# Data split\n",
    "TRAIN_RATIO = 0.8  # 80/20 train/val split\n",
    "\n",
    "# Checkpoint config\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "SAVE_EVERY = 10  # Save checkpoint every N epochs\n",
    "\n",
    "# Logging config\n",
    "LOG_DIR = Path('./logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  EMA momentum: {EMA_MOMENTUM}\")\n",
    "print(f\"  Image size: {IMG_SIZE}×{IMG_SIZE}\")\n",
    "print(f\"  Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"  Log dir: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30385d-ac2e-4412-922b-5a5ad67c8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINT MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with training state.\n",
    "    \n",
    "    Args:\n",
    "        model: BYOL model\n",
    "        optimizer: Optimizer\n",
    "        epoch: Current epoch number\n",
    "        loss: Current loss value\n",
    "        filepath: Path to save checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'online_encoder_state_dict': model.online_encoder.state_dict(),\n",
    "        'online_projector_state_dict': model.online_projector.state_dict(),\n",
    "        'predictor_state_dict': model.predictor.state_dict(),\n",
    "        'target_encoder_state_dict': model.target_encoder.state_dict(),\n",
    "        'target_projector_state_dict': model.target_projector.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"✓ Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint and resume training.\n",
    "    \n",
    "    Args:\n",
    "        model: BYOL model\n",
    "        optimizer: Optimizer\n",
    "        filepath: Path to checkpoint file\n",
    "    \n",
    "    Returns:\n",
    "        epoch: Epoch number to resume from\n",
    "        loss: Loss value at checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    model.online_encoder.load_state_dict(checkpoint['online_encoder_state_dict'])\n",
    "    model.online_projector.load_state_dict(checkpoint['online_projector_state_dict'])\n",
    "    model.predictor.load_state_dict(checkpoint['predictor_state_dict'])\n",
    "    model.target_encoder.load_state_dict(checkpoint['target_encoder_state_dict'])\n",
    "    model.target_projector.load_state_dict(checkpoint['target_projector_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"✓ Checkpoint loaded: {filepath}\")\n",
    "    print(f\"  Resuming from epoch {epoch}, loss: {loss:.4f}\")\n",
    "    \n",
    "    return epoch, loss\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOSS LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "class LossLogger:\n",
    "    \"\"\"Log training losses to CSV and plot them.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.csv_path = self.log_dir / f\"losses_{timestamp}.csv\"\n",
    "        self.plot_path = self.log_dir / f\"loss_curve_{timestamp}.png\"\n",
    "        \n",
    "        # Initialize CSV\n",
    "        with open(self.csv_path, 'w') as f:\n",
    "            f.write(\"epoch,batch,loss\\n\")\n",
    "        \n",
    "        self.losses = []\n",
    "    \n",
    "    def log_batch(self, epoch, batch, loss):\n",
    "        \"\"\"Log loss for a single batch.\"\"\"\n",
    "        with open(self.csv_path, 'a') as f:\n",
    "            f.write(f\"{epoch},{batch},{loss:.6f}\\n\")\n",
    "        \n",
    "        self.losses.append(loss)\n",
    "    \n",
    "    def log_epoch(self, epoch, avg_loss):\n",
    "        \"\"\"Log average loss for an epoch.\"\"\"\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS} - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot loss curve and save to file.\"\"\"\n",
    "        if len(self.losses) == 0:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses, alpha=0.6, label='Batch loss')\n",
    "        \n",
    "        # Smooth with moving average (window=100)\n",
    "        if len(self.losses) > 100:\n",
    "            window = 100\n",
    "            smoothed = pd.Series(self.losses).rolling(window=window, center=True).mean()\n",
    "            plt.plot(smoothed, linewidth=2, label=f'Smoothed (window={window})')\n",
    "        \n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('BYOL Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plot_path, dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✓ Loss curve saved: {self.plot_path}\")\n",
    "\n",
    "\n",
    "# Initialize logger\n",
    "logger = LossLogger(LOG_DIR)\n",
    "print(f\"✓ Loss logger initialized\")\n",
    "print(f\"  CSV: {logger.csv_path}\")\n",
    "print(f\"  Plot: {logger.plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7136e9-29a0-47f2-ba84-6fc2d174a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LEARNING RATE SCHEDULER (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment to use cosine annealing learning rate schedule\n",
    "# This gradually reduces learning rate from initial to 0 over training\n",
    "\n",
    "USE_LR_SCHEDULER = False  # Set to True to enable\n",
    "\n",
    "if USE_LR_SCHEDULER:\n",
    "    # Note: scheduler will be created after optimizer is defined\n",
    "    print(\"✓ Learning rate scheduler: Cosine annealing (enabled)\")\n",
    "    print(f\"  LR will decay from {LEARNING_RATE} to 0 over {NUM_EPOCHS} epochs\")\n",
    "else:\n",
    "    print(\"✓ Learning rate scheduler: None (constant LR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3e50b-51e5-4f9c-9ffa-f27a407dfff7",
   "metadata": {},
   "source": [
    "## Augmentation Strategy: Multi-Scheme Label-Based Positive Pairs\n",
    "\n",
    "### Standard BYOL:\n",
    "- Positive pair = two augmentations of the **same image**\n",
    "\n",
    "### Our approach (Multi-Scheme Semantic Similarity):\n",
    "1. **Randomly select** a classification scheme (Initial/Morphology/Environment/Derived)\n",
    "2. **Randomly select** a non-zero label within that scheme\n",
    "3. **Sample two different images** that share that label\n",
    "4. Apply **independent spatial augmentations** to each image\n",
    "\n",
    "### Example positive pairs:\n",
    "- Two FRIIs (Initial classification)\n",
    "- Two X-shaped galaxies (Morphology)\n",
    "- Two galaxies in clusters (Environment)\n",
    "- Two curved FRIs (Derived)\n",
    "\n",
    "This teaches the network **multiple valid notions of similarity**, creating a richer latent space.\n",
    "\n",
    "### Spatial Augmentations:\n",
    "- Random crops (80-100%)\n",
    "- Random flips (H/V)\n",
    "- Random rotations (0-360°)\n",
    "- Gaussian blur\n",
    "- No colour jitter (single-channel radio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe72b7-80ac-4197-9df4-df0652ac286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BYOL CONFIGURATION: Multi-Scheme Positive Pairs\n",
    "# =============================================================================\n",
    "\n",
    "# Which classification schemes to use for positive pair sampling\n",
    "# Set to None to use all schemes, or list specific columns [0, 1, 2, 3]\n",
    "USE_SCHEMES = None  # None = use all 4 schemes randomly\n",
    "# USE_SCHEMES = [0, 1]  # Only use Initial + Morphology\n",
    "\n",
    "# Probability of sampling from each scheme (if None, uniform probability)\n",
    "SCHEME_WEIGHTS = None  # None = equal probability for each scheme\n",
    "# SCHEME_WEIGHTS = [0.5, 0.3, 0.1, 0.1]  # Weight towards Initial classification\n",
    "\n",
    "print(\"✓ Multi-scheme positive pair configuration:\")\n",
    "if USE_SCHEMES is None:\n",
    "    print(\"  Using all 4 classification schemes\")\n",
    "else:\n",
    "    scheme_names = ['Initial', 'Morphology', 'Environment', 'Derived']\n",
    "    print(f\"  Using schemes: {[scheme_names[i] for i in USE_SCHEMES]}\")\n",
    "\n",
    "if SCHEME_WEIGHTS is None:\n",
    "    print(\"  Sampling uniformly across schemes\")\n",
    "else:\n",
    "    print(f\"  Scheme weights: {SCHEME_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc3296-d776-49c9-b958-b2403eb57df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUGMENTATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "augmentation_pipeline = T.Compose([\n",
    "    #T.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=180),\n",
    "    T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "print(\"✓ Spatial augmentation pipeline:\")\n",
    "print(\"  - RandomResizedCrop(89, scale=(0.8, 1.0))\")\n",
    "print(\"  - RandomHorizontalFlip(p=0.5)\")\n",
    "print(\"  - RandomVerticalFlip(p=0.5)\")\n",
    "print(\"  - RandomRotation(degrees=180)\")\n",
    "print(\"  - GaussianBlur(kernel_size=9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37214781-74dc-41c5-abf6-420cfabb48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTI-SCHEME DATASET: Dynamic Label-Based Positive Pairs\n",
    "# =============================================================================\n",
    "\n",
    "class MultiSchemeRadioGalaxyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for BYOL with multi-scheme label-based positive pairs.\n",
    "    \n",
    "    Randomly selects a classification scheme and samples two different images\n",
    "    that share the same non-zero label in that scheme.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None, \n",
    "                 use_schemes=None, scheme_weights=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: numpy array (N, 89, 89)\n",
    "            labels: numpy array (N, 4) with multi-label classifications\n",
    "            transform: torchvision transforms\n",
    "            use_schemes: list of scheme indices to use, or None for all\n",
    "            scheme_weights: probability weights for each scheme, or None for uniform\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Determine which schemes to use\n",
    "        if use_schemes is None:\n",
    "            self.use_schemes = [0, 1, 2, 3]  # All schemes\n",
    "        else:\n",
    "            self.use_schemes = use_schemes\n",
    "        \n",
    "        # Set sampling weights\n",
    "        if scheme_weights is None:\n",
    "            self.scheme_weights = [1.0 / len(self.use_schemes)] * len(self.use_schemes)\n",
    "        else:\n",
    "            assert len(scheme_weights) == len(self.use_schemes)\n",
    "            total = sum(scheme_weights)\n",
    "            self.scheme_weights = [w / total for w in scheme_weights]\n",
    "        \n",
    "        # Build label-to-indices mapping for EACH scheme\n",
    "        # Structure: scheme_indices[scheme_col][label_value] = [list of sample indices]\n",
    "        self.scheme_indices = {}\n",
    "        \n",
    "        for scheme_col in self.use_schemes:\n",
    "            self.scheme_indices[scheme_col] = {}\n",
    "            \n",
    "            for idx in range(len(labels)):\n",
    "                label_value = labels[idx, scheme_col]\n",
    "                \n",
    "                # Only store non-zero labels (0 = N/A)\n",
    "                if label_value != 0:\n",
    "                    if label_value not in self.scheme_indices[scheme_col]:\n",
    "                        self.scheme_indices[scheme_col][label_value] = []\n",
    "                    self.scheme_indices[scheme_col][label_value].append(idx)\n",
    "        \n",
    "        # Pre-compute valid indices (samples with at least one non-zero label)\n",
    "        self.valid_indices = []\n",
    "        for idx in range(len(images)):\n",
    "            # Check if this sample has at least one non-zero label in use_schemes\n",
    "            if any(labels[idx, col] != 0 for col in self.use_schemes):\n",
    "                self.valid_indices.append(idx)\n",
    "        \n",
    "        print(f\"  Dataset initialized:\")\n",
    "        print(f\"    Total samples: {len(self.images)}\")\n",
    "        print(f\"    Valid samples: {len(self.valid_indices)} (have ≥1 non-zero label)\")\n",
    "        print(f\"    Active schemes: {len(self.use_schemes)}\")\n",
    "        \n",
    "        # Print statistics for each scheme\n",
    "        scheme_names = ['Initial', 'Morphology', 'Environment', 'Derived']\n",
    "        for scheme_col in self.use_schemes:\n",
    "            n_labels = len(self.scheme_indices[scheme_col])\n",
    "            n_samples = sum(len(indices) for indices in self.scheme_indices[scheme_col].values())\n",
    "            print(f\"      {scheme_names[scheme_col]:12s} (col {scheme_col}): \"\n",
    "                  f\"{n_labels} labels, {n_samples} labeled samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns two different images that share the same label in a randomly\n",
    "        selected classification scheme.\n",
    "        \"\"\"\n",
    "        # Map to valid index\n",
    "        true_idx = self.valid_indices[idx]\n",
    "        \n",
    "        # Get labels for this sample\n",
    "        sample_labels = self.labels[true_idx]\n",
    "        \n",
    "        # Find which schemes have non-zero labels for this sample\n",
    "        available_schemes = [col for col in self.use_schemes \n",
    "                           if sample_labels[col] != 0]\n",
    "        \n",
    "        if len(available_schemes) == 0:\n",
    "            # Should not happen due to valid_indices filtering, but safety check\n",
    "            raise ValueError(f\"Sample {true_idx} has no non-zero labels\")\n",
    "        \n",
    "        # Randomly select a scheme (weighted if specified)\n",
    "        if len(available_schemes) < len(self.use_schemes):\n",
    "            # Some schemes unavailable for this sample, uniform selection\n",
    "            scheme_col = np.random.choice(available_schemes)\n",
    "        else:\n",
    "            # All schemes available, use weights\n",
    "            scheme_col = np.random.choice(self.use_schemes, p=self.scheme_weights)\n",
    "            # Check if selected scheme is actually available\n",
    "            if sample_labels[scheme_col] == 0:\n",
    "                # Fall back to any available scheme\n",
    "                scheme_col = np.random.choice(available_schemes)\n",
    "        \n",
    "        # Get the label value for this scheme\n",
    "        label_value = sample_labels[scheme_col]\n",
    "        \n",
    "        # Get first image\n",
    "        img1 = self.images[true_idx]\n",
    "        \n",
    "        # Sample a DIFFERENT image with the same label in this scheme\n",
    "        same_label_indices = self.scheme_indices[scheme_col][label_value]\n",
    "        \n",
    "        if len(same_label_indices) > 1:\n",
    "            # Choose different image\n",
    "            other_indices = [i for i in same_label_indices if i != true_idx]\n",
    "            idx2 = np.random.choice(other_indices)\n",
    "            img2 = self.images[idx2]\n",
    "        else:\n",
    "            # Only one image with this label (rare), use same image\n",
    "            img2 = img1\n",
    "        \n",
    "        # Convert to tensors (H, W) -> (1, H, W)\n",
    "        img1 = torch.from_numpy(img1).float().unsqueeze(0)\n",
    "        img2 = torch.from_numpy(img2).float().unsqueeze(0)\n",
    "        \n",
    "        # Apply independent augmentations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        return img1, img2\n",
    "\n",
    "\n",
    "print(\"✓ MultiSchemeRadioGalaxyDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5e9c4-fb24-414f-be65-8ace73e3fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = MultiSchemeRadioGalaxyDataset(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    transform=augmentation_pipeline,\n",
    "    use_schemes=USE_SCHEMES,\n",
    "    scheme_weights=SCHEME_WEIGHTS\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = MultiSchemeRadioGalaxyDataset(\n",
    "    val_images,\n",
    "    val_labels,\n",
    "    transform=augmentation_pipeline,\n",
    "    use_schemes=USE_SCHEMES,\n",
    "    scheme_weights=SCHEME_WEIGHTS\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc134709-cd9b-4c7e-8fb4-9715af1cd957",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train BYOL with multi-scheme label-based positive pairs.\n",
    "\n",
    "### Per-Iteration Process:\n",
    "```\n",
    "For each batch:\n",
    "  1. Dataset samples anchor image (idx)\n",
    "  2. Randomly select classification scheme (Initial/Morphology/Environment/Derived)\n",
    "  3. Find anchor's label in that scheme\n",
    "  4. Sample different image with same label → positive pair\n",
    "  5. Apply independent augmentations (crops, flips, rotations, blur) to both\n",
    "  6. Forward pass: online network predicts target network's representations\n",
    "  7. Compute BYOL loss: L = MSE(p1, t2) + MSE(p2, t1)\n",
    "  8. Backward pass + optimizer step\n",
    "  9. Update target network via EMA: θ_target = 0.99·θ_target + 0.01·θ_online\n",
    "```\n",
    "\n",
    "### Monitoring:\n",
    "- Loss logged every batch → CSV file\n",
    "- Checkpoint saved every 10 epochs\n",
    "- Loss curve plotted after training\n",
    "- Optional: Embedding visualization every 20 epochs (UMAP)\n",
    "\n",
    "### Notes:\n",
    "- **No pre-augmentation**: All transformations computed on-the-fly\n",
    "- **Dynamic positive pairs**: Each epoch sees different pairs/schemes\n",
    "- **Target network**: Updated via EMA (momentum=0.99), never backpropagated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8d603a-1d07-4d5b-a7b5-96822ec44614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training components...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing training components...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create DataLoaders\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(\n\u001b[1;32m      9\u001b[0m     train_dataset,\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     11\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     13\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Drop incomplete last batch for consistent training\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     18\u001b[0m     val_dataset,\n\u001b[1;32m     19\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ DataLoaders created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INITIALIZE MODEL, OPTIMIZER, DATALOADERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Initializing training components...\\n\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    drop_last=True  # Drop incomplete last batch for consistent training\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"✓ DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "\n",
    "# Create fresh backbone for BYOL\n",
    "print(f\"\\nCreating BYOL model...\")\n",
    "backbone = create_efficientnet_b0(num_channels=1, img_size=IMG_SIZE).to(device)\n",
    "\n",
    "model = BYOL(\n",
    "    backbone,\n",
    "    projection_dim=PROJECTION_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    img_size=IMG_SIZE\n",
    ").to(device)\n",
    "\n",
    "# Optimizer: ALL online network parameters\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.online_encoder.parameters()) +\n",
    "    list(model.online_projector.parameters()) +\n",
    "    list(model.predictor.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "\n",
    "# Optional: Learning rate scheduler\n",
    "scheduler = None\n",
    "if USE_LR_SCHEDULER:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=NUM_EPOCHS,\n",
    "        eta_min=0\n",
    "    )\n",
    "    print(f\"✓ LR Scheduler: CosineAnnealingLR\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"READY TO TRAIN\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model:              BYOL (EfficientNet-B0 from scratch)\")\n",
    "print(f\"Total params:       {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable params:   {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
    "print(f\"Batch size:         {BATCH_SIZE}\")\n",
    "print(f\"Train samples:      {len(train_dataset)}\")\n",
    "print(f\"Val samples:        {len(val_dataset)}\")\n",
    "print(f\"Steps per epoch:    {len(train_loader)}\")\n",
    "print(f\"Total epochs:       {NUM_EPOCHS}\")\n",
    "print(f\"Total steps:        {NUM_EPOCHS * len(train_loader)}\")\n",
    "print(f\"Device:             {device}\")\n",
    "print(f\"Positive pairs:     Multi-scheme (on-the-fly)\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e719c0a2-26ed-46d9-8dbb-c8e884f774b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NUM_EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Main training loop\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mNUM_EPOCHS\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# TRAINING PHASE\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"STARTING TRAINING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# Training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TRAINING PHASE\n",
    "    # =========================================================================\n",
    "    model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Train]\")\n",
    "    for batch_idx, (x1, x2) in enumerate(pbar):\n",
    "        # Move to device\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        \n",
    "        # Forward pass: online and target networks\n",
    "        p1, p2, t1, t2 = model(x1, x2)\n",
    "        \n",
    "        # Compute BYOL loss\n",
    "        loss = byol_loss(p1, p2, t1, t2)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update target network via EMA\n",
    "        model.update_target_network(momentum=EMA_MOMENTUM)\n",
    "        \n",
    "        # Record loss\n",
    "        loss_val = loss.item()\n",
    "        train_loss_epoch += loss_val\n",
    "        logger.log_batch(epoch, batch_idx, loss_val)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss_val:.4f}',\n",
    "            'avg': f'{train_loss_epoch / (batch_idx + 1):.4f}'\n",
    "        })\n",
    "    \n",
    "    # Average training loss\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # VALIDATION PHASE\n",
    "    # =========================================================================\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x1, x2 in tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Val]  \"):\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            \n",
    "            p1, p2, t1, t2 = model(x1, x2)\n",
    "            loss = byol_loss(p1, p2, t1, t2)\n",
    "            val_loss_epoch += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOGGING & CHECKPOINTING\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Record metrics\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Epoch time\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} Summary:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Train Loss:  {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:    {avg_val_loss:.4f}\")\n",
    "    print(f\"  LR:          {current_lr:.6f}\")\n",
    "    print(f\"  Time:        {epoch_time:.1f}s\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch % SAVE_EVERY == 0 or epoch == NUM_EPOCHS:\n",
    "        checkpoint_path = CHECKPOINT_DIR / f'byol_epoch_{epoch:03d}.pt'\n",
    "        save_checkpoint(model, optimizer, epoch, avg_train_loss, checkpoint_path)\n",
    "        print()\n",
    "\n",
    "# Training complete\n",
    "total_time = time.time() - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total time:       {hours}h {minutes}m\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Best val loss:    {min(history['val_loss']):.4f} (epoch {np.argmin(history['val_loss']) + 1})\")\n",
    "print(f\"Checkpoints:      {CHECKPOINT_DIR}\")\n",
    "print(f\"Logs:             {LOG_DIR}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Plot final loss curves\n",
    "logger.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff47cf9-b195-4fbc-b9fa-a4ca3718a5d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# PLOT TRAINING HISTORY\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Loss curves\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['epoch'], history['train_loss'], 'o-', label='Train Loss', linewidth=2)\n",
    "ax.plot(history['epoch'], history['val_loss'], 's-', label='Val Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('BYOL Loss')\n",
    "ax.set_title('Training & Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "ax = axes[1]\n",
    "ax.plot(history['epoch'], history['learning_rate'], 'o-', color='green', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(LOG_DIR / 'training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training history saved to {LOG_DIR / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57054d8-46c4-4c04-ba00-f4d80af01219",
   "metadata": {},
   "source": [
    "## Extract Embeddings for Evaluation\n",
    "\n",
    "After training, extract feature embeddings from the trained encoder for all datasets (train/val/test).\n",
    "\n",
    "These embeddings will be used for:\n",
    "- Visualization (UMAP/t-SNE)\n",
    "- Downstream tasks (classification, clustering)\n",
    "- Semantic interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a8cdab-83fd-42a2-a974-0db7e4bca44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for all datasets...\n",
      "\n",
      "  Train set:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Extract for train\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Train set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m train_embeddings, train_projections \u001b[38;5;241m=\u001b[39m extract_embeddings(\u001b[43mmodel\u001b[49m, train_images, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Projections: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_projections\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACT EMBEDDINGS\n",
    "# =============================================================================\n",
    "\n",
    "def extract_embeddings(model, images, batch_size=64):\n",
    "    \"\"\"\n",
    "    Extract feature embeddings from trained BYOL encoder.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BYOL model\n",
    "        images: numpy array (N, 89, 89)\n",
    "        batch_size: batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array (N, 1280) - features from backbone\n",
    "        projections: numpy array (N, 256) - projected features\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_projections = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=\"Extracting embeddings\"):\n",
    "            batch = images[i:i+batch_size]\n",
    "            \n",
    "            # Convert to tensor (N, H, W) -> (N, 1, H, W)\n",
    "            batch_tensor = torch.from_numpy(batch).float().unsqueeze(1).to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            features = model.online_encoder(batch_tensor)  # (N, 1280)\n",
    "            projections = model.online_projector(features)  # (N, 256)\n",
    "            \n",
    "            all_embeddings.append(features.cpu().numpy())\n",
    "            all_projections.append(projections.cpu().numpy())\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    projections = np.vstack(all_projections)\n",
    "    \n",
    "    return embeddings, projections\n",
    "\n",
    "\n",
    "print(\"Extracting embeddings for all datasets...\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"\\n  Train set:\")\n",
    "train_embeddings, train_projections = extract_embeddings(model, train_images, batch_size=64)\n",
    "print(f\"    Embeddings: {train_embeddings.shape}\")\n",
    "print(f\"    Projections: {train_projections.shape}\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"\\n  Val set:\")\n",
    "val_embeddings, val_projections = extract_embeddings(model, val_images, batch_size=64)\n",
    "print(f\"    Embeddings: {val_embeddings.shape}\")\n",
    "print(f\"    Projections: {val_projections.shape}\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"\\n  Test set:\")\n",
    "test_embeddings, test_projections = extract_embeddings(model, test_images, batch_size=64)\n",
    "print(f\"    Embeddings: {test_embeddings.shape}\")\n",
    "print(f\"    Projections: {test_projections.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "output_dir = Path('./embeddings')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(output_dir / 'train_embeddings.npy', train_embeddings)\n",
    "np.save(output_dir / 'train_projections.npy', train_projections)\n",
    "np.save(output_dir / 'val_embeddings.npy', val_embeddings)\n",
    "np.save(output_dir / 'val_projections.npy', val_projections)\n",
    "np.save(output_dir / 'test_embeddings.npy', test_embeddings)\n",
    "np.save(output_dir / 'test_projections.npy', test_projections)\n",
    "\n",
    "print(f\"\\n✓ Embeddings saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57c0279-c649-4152-8834-11d52bd29833",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# VISUALIZE LATENT SPACE WITH UMAP\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing UMAP projection for validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Use projection features (256-dim) for visualization\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE LATENT SPACE WITH UMAP\n",
    "# =============================================================================\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "print(\"Computing UMAP projection for validation set...\")\n",
    "\n",
    "# Use projection features (256-dim) for visualization\n",
    "reducer = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "val_umap = reducer.fit_transform(val_projections)\n",
    "\n",
    "print(f\"✓ UMAP computed: {val_umap.shape}\")\n",
    "\n",
    "# Plot UMAP colored by each classification scheme\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "scheme_names_list = ['Initial Classification', 'Morphology', 'Environment', 'Derived']\n",
    "label_maps = [INITIAL_CLASS_NAMES, MORPHOLOGY_NAMES, ENVIRONMENT_NAMES, DERIVED_NAMES]\n",
    "\n",
    "for scheme_idx, (ax, scheme_name, label_map) in enumerate(zip(axes, scheme_names_list, label_maps)):\n",
    "    \n",
    "    # Get labels for this scheme\n",
    "    scheme_labels = val_labels[:, scheme_idx]\n",
    "    \n",
    "    # Plot\n",
    "    scatter = ax.scatter(\n",
    "        val_umap[:, 0], \n",
    "        val_umap[:, 1],\n",
    "        c=scheme_labels,\n",
    "        cmap='tab20',\n",
    "        s=10,\n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'UMAP: Colored by {scheme_name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    \n",
    "    # Add colorbar with label names\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Class ID')\n",
    "\n",
    "plt.suptitle('BYOL Latent Space Visualization (Validation Set)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(LOG_DIR / 'latent_space_umap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ UMAP visualization saved to {LOG_DIR / 'latent_space_umap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987ce27-97fd-417c-92f2-df32b6e20f3e",
   "metadata": {},
   "source": [
    "TODO: Save and export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-GPU (PyTorch)",
   "language": "python",
   "name": "astro-gpu-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
