{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067a895d-6081-4114-a848-2b6f88b8713c",
   "metadata": {},
   "source": [
    "# BYOL Implementation for LOTSS DR2 Radio Galaxies\n",
    "\n",
    "**Project G: Interpretable Latent Space Semantics for Radio Galaxies**\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook creates an interpretable latent space for radio galaxy morphologies using a modified BYOL (Bootstrap Your Own Latent) architecture trained from scratch. The goal is to produce semantically meaningful embeddings where morphological properties vary smoothly, enabling interpolation and exploration of the latent space.\n",
    "\n",
    "## Method\n",
    "\n",
    "We adapt BYOL's contrastive learning approach using morphological classifications from Horton et al. (2025, A&A, 699, A338):\n",
    "\n",
    "**Standard BYOL**: Positive pairs = two augmentations of the same image  \n",
    "**Our modification**: Positive pairs = different images with the same morphological label\n",
    "\n",
    "This combines self-supervised learning with weak supervision to create a latent space where:\n",
    "- Similar morphologies cluster together (FRI, FRII, Hybrid, etc.)\n",
    "- Latent dimensions correspond to interpretable features\n",
    "- Smooth transitions exist between morphological classes\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- **Backbone**: EfficientNet-B0 (trained from scratch, no ImageNet pretraining)\n",
    "- **Projection head**: 1280 → 4096 → 256 dimensions\n",
    "- **Predictor head**: 256 → 4096 → 256 dimensions\n",
    "- **Input**: 89×89 greyscale numpy arrays (LOTSS DR2 cutouts)\n",
    "- **Output**: 256-dimensional embeddings\n",
    "- **Target network**: EMA updates with τ = 0.99\n",
    "\n",
    "## Data\n",
    "\n",
    "- Source: LoTSS DR2 (Shimwell et al. 2022)\n",
    "- Labels: Horton et al. (2025) morphological classifications\n",
    "  - 9,985 visually classified sources\n",
    "  - Classes: FRI (2406), FRII (4693), Hybrid (751), Relaxed doubles (361), etc.\n",
    "- Image size: 89×89 pixels (radio continuum at 144 MHz)\n",
    "\n",
    "## References\n",
    "\n",
    "- BYOL: Grill et al. (2020), NeurIPS 33\n",
    "- Morphology labels: Horton et al. (2025), A&A, 699, A338\n",
    "- LoTSS DR2: Shimwell et al. (2022), A&A, 659, A1\n",
    "\n",
    "## TO DO\n",
    "- Update label handelling\n",
    "- Testrun it all\n",
    "- More visualisation plots\n",
    "- Update address to the data\n",
    "- Save and export\n",
    "\n",
    "(Text generated with Claude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4189f1-7496-431d-9ded-97f862bec063",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cead9fa-efd0-4720-9af2-648abb911715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed)\n",
    "# !pip install torch torchvision astropy pandas numpy matplotlib tqdm --break-system-packages\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# Astronomical data\n",
    "from astropy.io import fits\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "import os\n",
    "os.chdir('/idia/users/markusbredberg/projectG_supervised_latent_radiogals/')\n",
    "from data_samplers import BYOLSupDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762581c-d7a4-47e4-a3af-c9255e9e7fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET DEVICE - FORCE GPU\n",
    "# =============================================================================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Force CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.set_device(0)  # Use first GPU\n",
    "    \n",
    "    print(f\"✓ Using device: {device}\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Clear any existing GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"⚠ CUDA not available, using CPU\")\n",
    "    print(f\"  This will be VERY slow and may crash with large batches\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"  CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dedaa-f950-4db2-a35f-ca60e51f0f38",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "- **Batch size**: (reduce to 16 if GPU memory issues)\n",
    "- **Epochs**:\n",
    "- **Learning rate**:\n",
    "- **Optimizer**: Adam\n",
    "- **EMA momentum (τ)**: 0.99 (target network update rate)\n",
    "\n",
    "### Data Strategy\n",
    "- **Positive pairs**: Different images with same morphological label\n",
    "- **Augmentations**: Crops, flips, rotations, Gaussian blur (no colour jitter)\n",
    "\n",
    "### Training Utilities\n",
    "- Checkpoint saving every 10 epochs\n",
    "- Loss logging to CSV\n",
    "- Optional: Cosine annealing LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf7380-d974-40e5-b85f-92d173f0cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Device-dependent settings\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    MOCK_DATA_SIZE = None  # Use full real data\n",
    "else:\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 5\n",
    "    MOCK_DATA_SIZE = 100  # Tiny dataset for CPU testing\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "EMA_MOMENTUM = 0.99\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING = True\n",
    "PATIENCE = 10\n",
    "MIN_DELTA = 0.001\n",
    "\n",
    "# Model\n",
    "PROJECTION_DIM = 256\n",
    "HIDDEN_DIM = 4096\n",
    "IMG_SIZE = 89\n",
    "\n",
    "# Checkpointing\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "SAVE_EVERY = 10\n",
    "\n",
    "LOG_DIR = Path('./logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  LR: {LEARNING_RATE}\")\n",
    "print(f\"  Early stopping: {EARLY_STOPPING} (patience={PATIENCE})\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(f\"  ⚠ CPU mode: Using tiny dataset ({MOCK_DATA_SIZE} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319347f2-2091-4114-846c-9552b5b72a46",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load LoTSS DR2 radio galaxy images (89×89 numpy arrays) and binary multi-label tag vectors.\n",
    "\n",
    "### Data Structure:\n",
    "- **Images**: `images.npy` - shape `(N, 89, 89)` greyscale radio continuum\n",
    "- **Labels**: `labels.npy` - shape `(N, num_classes)` binary multi-label vectors\n",
    "  - Each element is 0 or 1: `[0, 1, 0, 0, 0, 0, 1, 1, 0, 1, ...]`\n",
    "  - Value of 1 at index i means sample belongs to class i\n",
    "  - Samples can belong to multiple classes simultaneously\n",
    "  - Used for computing sample similarity via cityblock (Manhattan) distance\n",
    "  - Closer labels → more likely to be paired during training\n",
    "\n",
    "### Data Source:\n",
    "- LoTSS DR2 cutouts (Shimwell et al. 2022)\n",
    "- Location: `/users/markusbredberg/workspace/projectG_supervised_latent_radiogals/`\n",
    "- Binary classification tags for morphology, environment, and derived properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6ffc3-fe17-4725-b515-21903f3d9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = Path('/users/markusbredberg/workspace/projectG_supervised_latent_radiogals')\n",
    "IMAGES_PATH = DATA_DIR / 'images.npy'\n",
    "LABELS_PATH = DATA_DIR / 'labels.npy'\n",
    "\n",
    "print(\"Attempting to load real LOTSS data...\")\n",
    "print(f\"  Images: {IMAGES_PATH}\")\n",
    "print(f\"  Labels: {LABELS_PATH}\")\n",
    "\n",
    "# Check if files exist\n",
    "if not IMAGES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Images file not found: {IMAGES_PATH}\")\n",
    "if not LABELS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Labels file not found: {LABELS_PATH}\")\n",
    "\n",
    "# Load data\n",
    "images = np.load(IMAGES_PATH)\n",
    "labels = np.load(LABELS_PATH)\n",
    "print(\"Examples labels\", labels[:5])\n",
    "\n",
    "# Validate\n",
    "assert len(images) == len(labels), f\"Mismatch: {len(images)} images, {len(labels)} labels\"\n",
    "assert images.ndim == 3, f\"Expected 3D images, got {images.ndim}D: {images.shape}\"\n",
    "assert images.shape[1] == images.shape[2] == 89, f\"Expected 89×89, got {images.shape[1:3]}\"\n",
    "\n",
    "# CPU mode: subsample for speed\n",
    "if MOCK_DATA_SIZE is not None and len(images) > MOCK_DATA_SIZE:\n",
    "    print(f\"\\n⚠ CPU mode: Subsampling {MOCK_DATA_SIZE}/{len(images)} samples\")\n",
    "    indices = np.random.choice(len(images), MOCK_DATA_SIZE, replace=False)\n",
    "    images = images[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "print(f\"\\n✓ Real data loaded\")\n",
    "print(f\"  Images: {images.shape} ({images.dtype})\")\n",
    "print(f\"  Labels: {labels.shape} ({labels.dtype})\")\n",
    "print(f\"  Range: [{images.min():.2f}, {images.max():.2f}]\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN/VAL/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "print(f\"\\nSplitting data ({TRAIN_RATIO:.0%}/{VAL_RATIO:.0%}/{TEST_RATIO:.0%})...\")\n",
    "\n",
    "indices = np.arange(len(images))\n",
    "\n",
    "# Split\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices, test_size=(VAL_RATIO + TEST_RATIO), random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=TEST_RATIO/(VAL_RATIO+TEST_RATIO), random_state=42\n",
    ")\n",
    "\n",
    "train_images = images[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "val_images = images[val_idx]\n",
    "val_labels = labels[val_idx]\n",
    "test_images = images[test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "print(f\"  Train: {len(train_images)}\")\n",
    "print(f\"  Val:   {len(val_images)}\")\n",
    "print(f\"  Test:  {len(test_images)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "\n",
    "# Convert numpy arrays to DataFrames (required by BYOLSupDataset)\n",
    "train_labels_df = pd.DataFrame(train_labels)\n",
    "val_labels_df = pd.DataFrame(val_labels)\n",
    "test_labels_df = pd.DataFrame(test_labels)\n",
    "\n",
    "print(f\"  Converted labels to DataFrames\")\n",
    "\n",
    "# Transforms\n",
    "base_transform = T.Compose([\n",
    "    #T.ToTensor(),\n",
    "])\n",
    "\n",
    "byol_strong_aug = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(180),\n",
    "    #T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = BYOLSupDataset(\n",
    "    tags_data=train_labels_df,\n",
    "    img_data=train_images,\n",
    "    transform=base_transform,\n",
    "    friend_transform=byol_strong_aug,\n",
    "    p_pair_from_class=0.5\n",
    ")\n",
    "\n",
    "val_dataset = BYOLSupDataset(\n",
    "    tags_data=val_labels_df,\n",
    "    img_data=val_images,\n",
    "    transform=base_transform,\n",
    "    friend_transform=byol_strong_aug,\n",
    "    p_pair_from_class=0.5\n",
    ")\n",
    "\n",
    "test_dataset = BYOLSupDataset(\n",
    "    tags_data=test_labels_df,\n",
    "    img_data=test_images,\n",
    "    transform=base_transform,\n",
    "    friend_transform=byol_strong_aug,\n",
    "    p_pair_from_class=0.5\n",
    ")\n",
    "\n",
    "# DATA LOADERS \n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=4 if use_cuda else 0,\n",
    "    pin_memory=use_cuda, drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=4 if use_cuda else 0,\n",
    "    pin_memory=use_cuda\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=4 if use_cuda else 0,\n",
    "    pin_memory=use_cuda\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ REAL DATA LOADED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Train: {len(train_loader)} batches × {BATCH_SIZE}\")\n",
    "print(f\"Val:   {len(val_loader)} batches × {BATCH_SIZE}\")\n",
    "print(f\"Test:  {len(test_loader)} batches × {BATCH_SIZE}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Test sampling\n",
    "x1, x2, _ = next(iter(train_loader))\n",
    "print(f\"✓ Test batch: {x1.shape}, {x2.shape}\")\n",
    "print(f\"  Different: {not torch.allclose(x1, x2)}\")\n",
    "\n",
    "try:\n",
    "    a=1\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ REAL DATA LOADING FAILED\")\n",
    "    print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "    print(f\"\\n→ Falling back to MOCK data...\\n\")\n",
    "    \n",
    "    # Generate mock data\n",
    "    def generate_mock_dataloaders(batch_size=32, n_train=320, n_val=64, n_test=64):\n",
    "        class MockPairDataset(Dataset):\n",
    "            def __init__(self, n_samples):\n",
    "                self.n_samples = n_samples\n",
    "                self.transform = T.Compose([\n",
    "                    T.RandomHorizontalFlip(p=0.5),\n",
    "                    T.RandomVerticalFlip(p=0.5),\n",
    "                    T.RandomRotation(degrees=180),\n",
    "                ])\n",
    "            \n",
    "            def __len__(self):\n",
    "                return self.n_samples\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                x1 = torch.randn(1, 89, 89)\n",
    "                x2 = torch.randn(1, 89, 89)\n",
    "                return self.transform(x1), self.transform(x2)\n",
    "        \n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        \n",
    "        loaders = []\n",
    "        for n in [n_train, n_val, n_test]:\n",
    "            dataset = MockPairDataset(n)\n",
    "            loader = DataLoader(\n",
    "                dataset, batch_size=batch_size,\n",
    "                shuffle=(n==n_train), num_workers=0,\n",
    "                pin_memory=use_cuda, drop_last=(n==n_train)\n",
    "            )\n",
    "            loaders.append(loader)\n",
    "        \n",
    "        return loaders\n",
    "    \n",
    "    # Generate with appropriate sizes\n",
    "    if MOCK_DATA_SIZE:\n",
    "        n_train = int(MOCK_DATA_SIZE * 0.7)\n",
    "        n_val = int(MOCK_DATA_SIZE * 0.15)\n",
    "        n_test = int(MOCK_DATA_SIZE * 0.15)\n",
    "    else:\n",
    "        n_train, n_val, n_test = 320, 64, 64\n",
    "    \n",
    "    train_loader, val_loader, test_loader = generate_mock_dataloaders(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_train=n_train,\n",
    "        n_val=n_val,\n",
    "        n_test=n_test\n",
    "    )\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"✓ MOCK DATA LOADED\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train: {len(train_loader)} batches × {BATCH_SIZE}\")\n",
    "    print(f\"Val:   {len(val_loader)} batches × {BATCH_SIZE}\")\n",
    "    print(f\"Test:  {len(test_loader)} batches × {BATCH_SIZE}\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25560fb-84b4-4888-8886-d09cfaaa3d53",
   "metadata": {},
   "source": [
    "## BYOL Architecture (From Scratch)\n",
    "\n",
    "We implement BYOL without any pretrained weights or external libraries.\n",
    "\n",
    "### Components:\n",
    "1. **Backbone**: EfficientNet-B0 (1 channel input, 1280-dim output)\n",
    "2. **Projection MLP**: 1280 → 4096 → 256\n",
    "3. **Predictor MLP**: 256 → 4096 → 256\n",
    "4. **Target network**: EMA copy of encoder + projector (τ=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0f4ab-7861-4b41-a619-c1dadf753eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientnet_b0(num_channels=1, img_size=89):\n",
    "    \"\"\"\n",
    "    Create EfficientNet-B0 from scratch (no pretraining).\n",
    "    Modified for single-channel input (radio continuum).\n",
    "    \n",
    "    Args:\n",
    "        num_channels: Number of input channels (1 for greyscale radio)\n",
    "        img_size: Input image size (89 for LOTSS DR2 cutouts)\n",
    "    \n",
    "    Returns:\n",
    "        model: EfficientNet-B0 backbone with modified input and no classifier\n",
    "    \"\"\"\n",
    "    # Load model architecture WITHOUT ImageNet weights\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    \n",
    "    # Modify first conv layer for single-channel input\n",
    "    original_conv = model.features[0][0]\n",
    "    model.features[0][0] = nn.Conv2d(\n",
    "        num_channels,  # 1 channel instead of 3 (RGB)\n",
    "        original_conv.out_channels,\n",
    "        kernel_size=original_conv.kernel_size,\n",
    "        stride=original_conv.stride,\n",
    "        padding=original_conv.padding,\n",
    "        bias=False\n",
    "    )\n",
    "    \n",
    "    # Initialize the new conv layer (since we changed it)\n",
    "    nn.init.kaiming_normal_(model.features[0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    # Remove classification head (keep feature extractor only)\n",
    "    model.classifier = nn.Identity()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create and test backbone\n",
    "print(\"Creating EfficientNet-B0 backbone...\")\n",
    "backbone = create_efficientnet_b0(num_channels=1, img_size=89).to(device)\n",
    "\n",
    "# Test output dimension with 89x89 input\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(2, 1, 89, 89).to(device)\n",
    "    backbone_output = backbone(dummy_input)\n",
    "    backbone_dim = backbone_output.shape[1]\n",
    "\n",
    "print(f\"\\n✓ Backbone created successfully\")\n",
    "print(f\"  Architecture: EfficientNet-B0 (from scratch, no pretraining)\")\n",
    "print(f\"  Input shape: {dummy_input.shape}\")\n",
    "print(f\"  Output shape: {backbone_output.shape}\")\n",
    "print(f\"  Feature dimension: {backbone_dim}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in backbone.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76245ecd-71ab-482d-84f7-bc4b14785d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for projection and prediction heads.\n",
    "    Architecture: input_dim → hidden_dim (BN + ReLU) → output_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Test MLP dimensions\n",
    "print(\"Testing MLP architectures...\")\n",
    "\n",
    "# Projector: 1280 → 4096 → 256\n",
    "projector = MLP(input_dim=1280, hidden_dim=4096, output_dim=256)\n",
    "with torch.no_grad():\n",
    "    proj_test = projector(torch.randn(2, 1280))\n",
    "print(f\"✓ Projector: {1280} → {4096} → {256}, output shape: {proj_test.shape}\")\n",
    "\n",
    "# Predictor: 256 → 4096 → 256\n",
    "predictor = MLP(input_dim=256, hidden_dim=4096, output_dim=256)\n",
    "with torch.no_grad():\n",
    "    pred_test = predictor(torch.randn(2, 256))\n",
    "print(f\"✓ Predictor: {256} → {4096} → {256}, output shape: {pred_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53930b3-26d5-4eda-bf20-00a74827bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "    \"\"\"\n",
    "    Bootstrap Your Own Latent (BYOL)\n",
    "    \n",
    "    Two networks:\n",
    "    - Online network (trainable): encoder → projector → predictor\n",
    "    - Target network (EMA, frozen): encoder → projector\n",
    "    \n",
    "    The online network learns to predict the target network's representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, projection_dim=256, hidden_dim=4096, img_size=89):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Get backbone output dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, img_size, img_size)\n",
    "            if next(backbone.parameters()).is_cuda:\n",
    "                dummy = dummy.cuda()\n",
    "            backbone_dim = backbone(dummy).shape[1]\n",
    "        \n",
    "        print(f\"Backbone output dimension: {backbone_dim}\")\n",
    "        \n",
    "        # === ONLINE NETWORK (trainable) ===\n",
    "        self.online_encoder = backbone\n",
    "        self.online_projector = MLP(backbone_dim, hidden_dim, projection_dim)\n",
    "        self.predictor = MLP(projection_dim, hidden_dim, projection_dim)\n",
    "        \n",
    "        # === TARGET NETWORK (frozen, updated via EMA) ===\n",
    "        self.target_encoder = copy.deepcopy(backbone)\n",
    "        self.target_projector = copy.deepcopy(self.online_projector)\n",
    "        \n",
    "        # Freeze target network parameters\n",
    "        for param in self.target_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.target_projector.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass for two views.\n",
    "        \n",
    "        Args:\n",
    "            x1: First view (BATCH_SIZE, 1, 89, 89)\n",
    "            x2: Second view (BATCH_SIZE, 1, 89, 89)\n",
    "        \n",
    "        Returns:\n",
    "            p1, p2: Predictions from online network\n",
    "            t1, t2: Targets from target network (detached)\n",
    "        \"\"\"\n",
    "        # === ONLINE NETWORK ===\n",
    "        # Encode both views\n",
    "        z1_online = self.online_encoder(x1)  # (B, 1280)\n",
    "        z2_online = self.online_encoder(x2)  # (B, 1280)\n",
    "        \n",
    "        # Project to lower dimension\n",
    "        proj1_online = self.online_projector(z1_online)  # (B, 256)\n",
    "        proj2_online = self.online_projector(z2_online)  # (B, 256)\n",
    "        \n",
    "        # Predict target representations\n",
    "        p1 = self.predictor(proj1_online)  # (B, 256)\n",
    "        p2 = self.predictor(proj2_online)  # (B, 256)\n",
    "        \n",
    "        # === TARGET NETWORK (no gradients) ===\n",
    "        with torch.no_grad():\n",
    "            z1_target = self.target_encoder(x1)  # (B, 1280)\n",
    "            z2_target = self.target_encoder(x2)  # (B, 1280)\n",
    "            \n",
    "            t1 = self.target_projector(z1_target)  # (B, 256)\n",
    "            t2 = self.target_projector(z2_target)  # (B, 256)\n",
    "        \n",
    "        return p1, p2, t1, t2\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_target_network(self, momentum=0.99):\n",
    "        \"\"\"\n",
    "        Update target network using exponential moving average (EMA).\n",
    "        \n",
    "        θ_target = τ * θ_target + (1 - τ) * θ_online\n",
    "        \n",
    "        Args:\n",
    "            momentum: EMA decay rate (τ). Default: 0.99\n",
    "        \"\"\"\n",
    "        # Update target encoder\n",
    "        for online_param, target_param in zip(\n",
    "            self.online_encoder.parameters(), \n",
    "            self.target_encoder.parameters()\n",
    "        ):\n",
    "            target_param.data = momentum * target_param.data + (1 - momentum) * online_param.data\n",
    "        \n",
    "        # Update target projector\n",
    "        for online_param, target_param in zip(\n",
    "            self.online_projector.parameters(), \n",
    "            self.target_projector.parameters()\n",
    "        ):\n",
    "            target_param.data = momentum * target_param.data + (1 - momentum) * online_param.data\n",
    "\n",
    "\n",
    "# Test BYOL model\n",
    "print(\"\\nTesting BYOL model...\")\n",
    "test_model = BYOL(backbone, projection_dim=256, hidden_dim=4096, img_size=89)\n",
    "with torch.no_grad():\n",
    "    x1_test = torch.randn(2, 1, 89, 89)\n",
    "    x2_test = torch.randn(2, 1, 89, 89)\n",
    "    p1, p2, t1, t2 = test_model(x1_test, x2_test)\n",
    "\n",
    "print(f\"✓ BYOL model created\")\n",
    "print(f\"  Predictions (p1, p2): {p1.shape}, {p2.shape}\")\n",
    "print(f\"  Targets (t1, t2): {t1.shape}, {t2.shape}\")\n",
    "print(f\"  Target parameters frozen: {not next(test_model.target_encoder.parameters()).requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616ebac-dfcd-4866-9f9a-4ffcd1592678",
   "metadata": {},
   "source": [
    "### Loss function for BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d871d-e556-4043-a0ae-84f0fb0b8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byol_loss(p1, p2, t1, t2):\n",
    "    \"\"\"\n",
    "    BYOL loss function (symmetrised mean squared error on unit hypersphere).\n",
    "    \n",
    "    Loss = MSE(normalize(p1), normalize(t2)) + MSE(normalize(p2), normalize(t1))\n",
    "    \n",
    "    Equivalently (using cosine similarity):\n",
    "    Loss = 2 - 2 * [cos_sim(p1, t2) + cos_sim(p2, t1)]\n",
    "    \n",
    "    Args:\n",
    "        p1, p2: Predictions from online network (B, 256)\n",
    "        t1, t2: Targets from target network (B, 256)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss value (lower is better)\n",
    "    \"\"\"\n",
    "    # Normalize predictions and targets to unit hypersphere (L2 norm = 1)\n",
    "    p1 = F.normalize(p1, dim=-1, p=2)\n",
    "    p2 = F.normalize(p2, dim=-1, p=2)\n",
    "    t1 = F.normalize(t1, dim=-1, p=2)\n",
    "    t2 = F.normalize(t2, dim=-1, p=2)\n",
    "    \n",
    "    # Compute loss using cosine similarity\n",
    "    # We want high cosine similarity, so we minimize (2 - 2*cosine_similarity)\n",
    "    loss = 2 - 2 * (p1 * t2).sum(dim=-1).mean() - 2 * (p2 * t1).sum(dim=-1).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Alternative loss\n",
    "class NormalizedBYOLLoss(torch.nn.Module):    \n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z1, z2_t):\n",
    "        # L2-normalize in feature space\n",
    "        z1 = F.normalize(z1, dim=1)\n",
    "        z2_t = F.normalize(z2_t, dim=1)\n",
    "\n",
    "        # Temperature scaling\n",
    "        z1 = z1 / self.temperature\n",
    "        z2_t = z2_t / self.temperature\n",
    "\n",
    "        # Cosine similarity loss (BYOL-style)\n",
    "        return 2 - 2 * (z1 * z2_t).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "# Test loss function\n",
    "print(\"Testing BYOL loss...\")\n",
    "with torch.no_grad():\n",
    "    # Perfect predictions (should give loss ≈ 0)\n",
    "    loss_perfect = byol_loss(t1, t2, t1, t2)\n",
    "    print(f\"✓ Loss (perfect match): {loss_perfect.item():.6f} (should be ≈0)\")\n",
    "    \n",
    "    # Random predictions (should give loss ≈ 2)\n",
    "    random_p1 = torch.randn(2, 256)\n",
    "    random_p2 = torch.randn(2, 256)\n",
    "    loss_random = byol_loss(random_p1, random_p2, t1, t2)\n",
    "    print(f\"✓ Loss (random): {loss_random.item():.6f} (should be ≈2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30385d-ac2e-4412-922b-5a5ad67c8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHECKPOINT MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with training state.\n",
    "    \n",
    "    Args:\n",
    "        model: BYOL model\n",
    "        optimizer: Optimizer\n",
    "        epoch: Current epoch number\n",
    "        loss: Current loss value\n",
    "        filepath: Path to save checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'online_encoder_state_dict': model.online_encoder.state_dict(),\n",
    "        'online_projector_state_dict': model.online_projector.state_dict(),\n",
    "        'predictor_state_dict': model.predictor.state_dict(),\n",
    "        'target_encoder_state_dict': model.target_encoder.state_dict(),\n",
    "        'target_projector_state_dict': model.target_projector.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"✓ Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint and resume training.\n",
    "    \n",
    "    Args:\n",
    "        model: BYOL model\n",
    "        optimizer: Optimizer\n",
    "        filepath: Path to checkpoint file\n",
    "    \n",
    "    Returns:\n",
    "        epoch: Epoch number to resume from\n",
    "        loss: Loss value at checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    model.online_encoder.load_state_dict(checkpoint['online_encoder_state_dict'])\n",
    "    model.online_projector.load_state_dict(checkpoint['online_projector_state_dict'])\n",
    "    model.predictor.load_state_dict(checkpoint['predictor_state_dict'])\n",
    "    model.target_encoder.load_state_dict(checkpoint['target_encoder_state_dict'])\n",
    "    model.target_projector.load_state_dict(checkpoint['target_projector_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"✓ Checkpoint loaded: {filepath}\")\n",
    "    print(f\"  Resuming from epoch {epoch}, loss: {loss:.4f}\")\n",
    "    \n",
    "    return epoch, loss\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOSS LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "class LossLogger:\n",
    "    \"\"\"Log training losses to CSV and plot them.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.csv_path = self.log_dir / f\"losses_{timestamp}.csv\"\n",
    "        self.plot_path = self.log_dir / f\"loss_curve_{timestamp}.png\"\n",
    "        \n",
    "        # Initialize CSV\n",
    "        with open(self.csv_path, 'w') as f:\n",
    "            f.write(\"epoch,batch,loss\\n\")\n",
    "        \n",
    "        self.losses = []\n",
    "    \n",
    "    def log_batch(self, epoch, batch, loss):\n",
    "        \"\"\"Log loss for a single batch.\"\"\"\n",
    "        with open(self.csv_path, 'a') as f:\n",
    "            f.write(f\"{epoch},{batch},{loss:.6f}\\n\")\n",
    "        \n",
    "        self.losses.append(loss)\n",
    "    \n",
    "    def log_epoch(self, epoch, avg_loss):\n",
    "        \"\"\"Log average loss for an epoch.\"\"\"\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS} - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot loss curve and save to file.\"\"\"\n",
    "        if len(self.losses) == 0:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses, alpha=0.6, label='Batch loss')\n",
    "        \n",
    "        # Smooth with moving average (window=100)\n",
    "        if len(self.losses) > 100:\n",
    "            window = 100\n",
    "            smoothed = pd.Series(self.losses).rolling(window=window, center=True).mean()\n",
    "            plt.plot(smoothed, linewidth=2, label=f'Smoothed (window={window})')\n",
    "        \n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('BYOL Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plot_path, dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✓ Loss curve saved: {self.plot_path}\")\n",
    "\n",
    "\n",
    "# Initialize logger\n",
    "logger = LossLogger(LOG_DIR)\n",
    "print(f\"✓ Loss logger initialized\")\n",
    "print(f\"  CSV: {logger.csv_path}\")\n",
    "print(f\"  Plot: {logger.plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ad443-8bf2-4ef4-a83f-bbb44204920e",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae5084-8ddc-4101-8a78-cd758e080c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EARLY STOPPING\n",
    "# =============================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss stops improving.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.001, mode='min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait for improvement\n",
    "            min_delta: Minimum change to qualify as improvement\n",
    "            mode: 'min' for loss (lower is better), 'max' for accuracy\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, score, epoch):\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            score: Current validation metric (e.g., loss)\n",
    "            epoch: Current epoch number\n",
    "        \n",
    "        Returns:\n",
    "            improved: Whether this is a new best score\n",
    "        \"\"\"\n",
    "        if self.mode == 'min':\n",
    "            score = -score  # Convert to maximization problem\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            # First epoch\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return True\n",
    "        \n",
    "        if score > self.best_score + self.min_delta:\n",
    "            # Improvement\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "\n",
    "# Initialize early stopping\n",
    "if EARLY_STOPPING:\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA, mode='min')\n",
    "    print(f\"✓ Early stopping enabled (patience={PATIENCE}, min_delta={MIN_DELTA})\")\n",
    "else:\n",
    "    early_stopping = None\n",
    "    print(f\"✓ Early stopping disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7136e9-29a0-47f2-ba84-6fc2d174a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LEARNING RATE SCHEDULER (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment to use cosine annealing learning rate schedule\n",
    "# This gradually reduces learning rate from initial to 0 over training\n",
    "\n",
    "USE_LR_SCHEDULER = False  # Set to True to enable\n",
    "\n",
    "if USE_LR_SCHEDULER:\n",
    "    # Note: scheduler will be created after optimizer is defined\n",
    "    print(\"✓ Learning rate scheduler: Cosine annealing (enabled)\")\n",
    "    print(f\"  LR will decay from {LEARNING_RATE} to 0 over {NUM_EPOCHS} epochs\")\n",
    "else:\n",
    "    print(\"✓ Learning rate scheduler: None (constant LR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc134709-cd9b-4c7e-8fb4-9715af1cd957",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train BYOL with multi-scheme label-based positive pairs.\n",
    "\n",
    "### Per-Iteration Process:\n",
    "```\n",
    "For each batch:\n",
    "  1. Dataset samples anchor image (idx)\n",
    "  2. Randomly select classification scheme (Initial/Morphology/Environment/Derived)\n",
    "  3. Find anchor's label in that scheme\n",
    "  4. Sample different image with same label → positive pair\n",
    "  5. Apply independent augmentations (crops, flips, rotations, blur) to both\n",
    "  6. Forward pass: online network predicts target network's representations\n",
    "  7. Compute BYOL loss: L = MSE(p1, t2) + MSE(p2, t1)\n",
    "  8. Backward pass + optimizer step\n",
    "  9. Update target network via EMA: θ_target = 0.99·θ_target + 0.01·θ_online\n",
    "```\n",
    "\n",
    "### Monitoring:\n",
    "- Loss logged every batch → CSV file\n",
    "- Checkpoint saved every 10 epochs\n",
    "- Loss curve plotted after training\n",
    "- Optional: Embedding visualization every 20 epochs (UMAP)\n",
    "\n",
    "### Notes:\n",
    "- **No pre-augmentation**: All transformations computed on-the-fly\n",
    "- **Dynamic positive pairs**: Each epoch sees different pairs/schemes\n",
    "- **Target network**: Updated via EMA (momentum=0.99), never backpropagated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d603a-1d07-4d5b-a7b5-96822ec44614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INITIALIZE MODEL & OPTIMIZER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Initializing BYOL model...\\n\")\n",
    "\n",
    "# Create backbone\n",
    "backbone = create_efficientnet_b0(num_channels=1, img_size=IMG_SIZE).to(device)\n",
    "\n",
    "# Create BYOL model\n",
    "model = BYOL(\n",
    "    backbone,\n",
    "    projection_dim=PROJECTION_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    img_size=IMG_SIZE\n",
    ").to(device)\n",
    "\n",
    "# Optimizer: all online network parameters\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.online_encoder.parameters()) +\n",
    "    list(model.online_projector.parameters()) +\n",
    "    list(model.predictor.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Optional scheduler\n",
    "scheduler = None\n",
    "if USE_LR_SCHEDULER:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=NUM_EPOCHS, eta_min=0\n",
    "    )\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"MODEL SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Architecture:  BYOL (EfficientNet-B0 backbone)\")\n",
    "print(f\"Total params:  {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Trainable:     {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f}M\")\n",
    "print(f\"Optimizer:     Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Scheduler:     {'CosineAnnealing' if scheduler else 'None'}\")\n",
    "print(f\"Device:        {device}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719c0a2-26ed-46d9-8dbb-c8e884f774b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"STARTING TRAINING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    \n",
    "    # === TRAIN ===\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Train]\")\n",
    "    for x1, x2, _ in pbar:\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        p1, p2, t1, t2 = model(x1, x2)\n",
    "        loss = byol_loss(p1, p2, t1, t2)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # EMA update\n",
    "        model.update_target_network(momentum=EMA_MOMENTUM)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x1, x2, _ in tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Val]  \"):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            p1, p2, t1, t2 = model(x1, x2)\n",
    "            val_loss += byol_loss(p1, p2, t1, t2).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # === LOGGING ===\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"  LR:         {current_lr:.6f}\")\n",
    "    \n",
    "    # === BEST MODEL CHECKPOINT ===\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_model_path = CHECKPOINT_DIR / 'byol_best.pt'\n",
    "        save_checkpoint(model, optimizer, epoch, avg_val_loss, best_model_path)\n",
    "        print(f\"  ✓ New best model (val_loss={avg_val_loss:.4f})\")\n",
    "    \n",
    "    # === EARLY STOPPING CHECK ===\n",
    "    if early_stopping is not None:\n",
    "        improved = early_stopping(avg_val_loss, epoch)\n",
    "        if not improved:\n",
    "            print(f\"  No improvement for {early_stopping.counter}/{PATIENCE} epochs\")\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"EARLY STOPPING at epoch {epoch}\")\n",
    "            print(f\"  Best val loss: {best_val_loss:.4f} (epoch {best_epoch})\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            break\n",
    "    \n",
    "    # === LR SCHEDULER ===\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # === PERIODIC CHECKPOINT ===\n",
    "    if epoch % SAVE_EVERY == 0:\n",
    "        periodic_path = CHECKPOINT_DIR / f'byol_epoch_{epoch:03d}.pt'\n",
    "        save_checkpoint(model, optimizer, epoch, avg_train_loss, periodic_path)\n",
    "    \n",
    "    print()\n",
    "\n",
    "# === FINAL CHECKPOINT ===\n",
    "final_path = CHECKPOINT_DIR / 'byol_final.pt'\n",
    "save_checkpoint(model, optimizer, epoch, avg_train_loss, final_path)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Final epoch:      {epoch}\")\n",
    "print(f\"Best epoch:       {best_epoch}\")\n",
    "print(f\"Best val loss:    {best_val_loss:.4f}\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a4439-dbd1-4d41-a38f-1087389a7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD BEST MODEL FOR TEST EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading best model for test evaluation...\")\n",
    "\n",
    "best_model_path = CHECKPOINT_DIR / 'byol_best.pt'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    model.online_encoder.load_state_dict(checkpoint['online_encoder_state_dict'])\n",
    "    model.online_projector.load_state_dict(checkpoint['online_projector_state_dict'])\n",
    "    model.predictor.load_state_dict(checkpoint['predictor_state_dict'])\n",
    "    model.target_encoder.load_state_dict(checkpoint['target_encoder_state_dict'])\n",
    "    model.target_projector.load_state_dict(checkpoint['target_projector_state_dict'])\n",
    "    \n",
    "    print(f\"✓ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Val loss at checkpoint: {checkpoint['loss']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠ Best model not found, using final model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99605a09-feb5-4ac1-90de-0d9d0ab94f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST SET EVALUATION (HELD-OUT)\n",
    "# =============================================================================\n",
    "print(\"\\nEvaluating on TEST set (held-out)...\")\n",
    "\n",
    "# Check if test_loader exists from real data loading\n",
    "# If not, generate mock data\n",
    "if 'test_loader' not in locals():\n",
    "    print(\"→ test_loader not found, generating mock data...\")\n",
    "    \n",
    "    class MockPairDataset(Dataset):\n",
    "        def __init__(self, n_samples):\n",
    "            self.n_samples = n_samples\n",
    "            self.transform = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.RandomVerticalFlip(p=0.5),\n",
    "                T.RandomRotation(degrees=180),\n",
    "            ])\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.n_samples\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            x1 = torch.randn(1, 89, 89)\n",
    "            x2 = torch.randn(1, 89, 89)\n",
    "            mdist = 0.0  # Return 3 values to match real data\n",
    "            return self.transform(x1), self.transform(x2), mdist\n",
    "    \n",
    "    test_dataset = MockPairDataset(64)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False, \n",
    "        num_workers=0,\n",
    "        pin_memory=use_cuda\n",
    "    )\n",
    "    print(f\"  Created mock test_loader: {len(test_loader)} batches\")\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Unpack 3 values: x1, x2, mdist (ignore mdist)\n",
    "    for x1, x2, _ in tqdm(test_loader, desc=\"Test\"):\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        p1, p2, t1, t2 = model(x1, x2)\n",
    "        test_loss += byol_loss(p1, p2, t1, t2).item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST SET RESULTS (Best Model)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss:  {avg_test_loss:.4f}\")\n",
    "print(f\"Best Val:   {best_val_loss:.4f}\")\n",
    "print(f\"Difference: {abs(avg_test_loss - best_val_loss):.4f}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Add to history\n",
    "history['test_loss'] = avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d73682-9fe9-4c7e-adf3-d814721f314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT TRAINING HISTORY WITH EARLY STOPPING\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['epoch'], history['train_loss'], 'o-', label='Train', linewidth=2)\n",
    "ax.plot(history['epoch'], history['val_loss'], 's-', label='Val', linewidth=2)\n",
    "\n",
    "# Mark best epoch\n",
    "ax.axvline(best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best (epoch {best_epoch})')\n",
    "ax.scatter([best_epoch], [best_val_loss], color='red', s=100, zorder=5)\n",
    "\n",
    "# Test loss (horizontal line)\n",
    "if 'test_loss' in history:\n",
    "    ax.axhline(history['test_loss'], color='green', linestyle=':', \n",
    "               linewidth=2, label=f'Test (final)')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('BYOL Loss')\n",
    "ax.set_title('Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax = axes[1]\n",
    "ax.plot(history['epoch'], history['lr'], 'o-', color='green', linewidth=2)\n",
    "ax.axvline(best_epoch, color='red', linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(LOG_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved to {LOG_DIR / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57054d8-46c4-4c04-ba00-f4d80af01219",
   "metadata": {},
   "source": [
    "## Extract Embeddings for Evaluation\n",
    "\n",
    "After training, extract feature embeddings from the trained encoder for all datasets (train/val/test).\n",
    "\n",
    "These embeddings will be used for:\n",
    "- Visualization (UMAP/t-SNE)\n",
    "- Downstream tasks (classification, clustering)\n",
    "- Semantic interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8cdab-83fd-42a2-a974-0db7e4bca44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACT EMBEDDINGS (Works with Mock Data)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_embeddings_from_loader(model, dataloader, max_batches=None):\n",
    "    \"\"\"\n",
    "    Extract embeddings from a DataLoader.\n",
    "    Works with both mock and real data loaders.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BYOL model\n",
    "        dataloader: DataLoader that yields (x1, x2, mdist) tuples\n",
    "        max_batches: Limit number of batches (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: (N, 1280) features from backbone\n",
    "        projections: (N, 256) projected features\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_projections = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Unpack all 3 values from dataloader\n",
    "        for batch_idx, (x1, x2, _) in enumerate(tqdm(dataloader, desc=\"Extracting\")):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "            \n",
    "            # Use x1 (could use x2, doesn't matter - just need features)\n",
    "            x1 = x1.to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            features = model.online_encoder(x1)       # (B, 1280)\n",
    "            projections = model.online_projector(features)  # (B, 256)\n",
    "            \n",
    "            all_embeddings.append(features.cpu().numpy())\n",
    "            all_projections.append(projections.cpu().numpy())\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    projections = np.vstack(all_projections)\n",
    "    \n",
    "    return embeddings, projections\n",
    "\n",
    "\n",
    "print(\"Extracting embeddings from DataLoaders...\")\n",
    "\n",
    "# Extract from train loader (limit to 50 batches for speed)\n",
    "print(\"\\n  Train set:\")\n",
    "train_embeddings, train_projections = extract_embeddings_from_loader(\n",
    "    model, train_loader, max_batches=50\n",
    ")\n",
    "print(f\"    Embeddings: {train_embeddings.shape}\")\n",
    "print(f\"    Projections: {train_projections.shape}\")\n",
    "\n",
    "# Extract from val loader (all batches - it's small)\n",
    "print(\"\\n  Val set:\")\n",
    "val_embeddings, val_projections = extract_embeddings_from_loader(\n",
    "    model, val_loader, max_batches=None\n",
    ")\n",
    "print(f\"    Embeddings: {val_embeddings.shape}\")\n",
    "print(f\"    Projections: {val_projections.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "output_dir = Path('./embeddings')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(output_dir / 'train_embeddings.npy', train_embeddings)\n",
    "np.save(output_dir / 'train_projections.npy', train_projections)\n",
    "np.save(output_dir / 'val_embeddings.npy', val_embeddings)\n",
    "np.save(output_dir / 'val_projections.npy', val_projections)\n",
    "\n",
    "print(f\"\\n✓ Embeddings saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c0279-c649-4152-8834-11d52bd29833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIMENSIONALITY REDUCTION & VISUALIZATION\n",
    "# =============================================================================\n",
    "from sklearn.manifold import TSNE\n",
    "#import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\nPreparing embeddings and labels for visualization...\")\n",
    "\n",
    "# Extract embeddings if not already done\n",
    "# Use train embeddings and corresponding labels\n",
    "if 'train_embeddings' not in locals():\n",
    "    print(\"  Extracting embeddings...\")\n",
    "    train_embeddings, train_projections = extract_embeddings_from_loader(\n",
    "        model, train_loader, max_batches=50\n",
    "    )\n",
    "\n",
    "# Get corresponding labels for the extracted embeddings\n",
    "# Assuming we extracted first 50 batches\n",
    "n_samples = len(train_embeddings)\n",
    "train_labels_subset = train_labels[:n_samples]  # Match embedding count\n",
    "\n",
    "print(f\"  Embeddings: {train_embeddings.shape}\")\n",
    "print(f\"  Labels: {train_labels_subset.shape}\")\n",
    "print(f\"  Sample labels:\\n{train_labels_subset[:5]}\")\n",
    "\n",
    "# Compute label statistics for coloring\n",
    "label_counts = train_labels_subset.sum(axis=1)  # Number of active classes per sample\n",
    "dominant_class = train_labels_subset.argmax(axis=1)  # Primary class index\n",
    "\n",
    "print(f\"\\n  Label counts range: {label_counts.min():.0f} - {label_counts.max():.0f}\")\n",
    "print(f\"  Unique classes: {np.unique(dominant_class)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# t-SNE PROJECTION\n",
    "# =============================================================================\n",
    "print(\"\\nComputing t-SNE projection...\")\n",
    "tsne_model = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "tsne = tsne_model.fit_transform(train_embeddings)\n",
    "print(f\"  ✓ t-SNE shape: {tsne.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION: t-SNE\n",
    "# =============================================================================\n",
    "print(\"\\nCreating t-SNE visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('BYOL Latent Space - t-SNE Projection', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Number of active classes\n",
    "scatter1 = axes[0, 0].scatter(\n",
    "    tsne[:, 0], tsne[:, 1], \n",
    "    c=label_counts, \n",
    "    cmap='viridis',\n",
    "    s=30, \n",
    "    alpha=0.7,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[0, 0].set_title('Number of Active Classes', fontsize=12)\n",
    "axes[0, 0].set_xlabel('t-SNE 1')\n",
    "axes[0, 0].set_ylabel('t-SNE 2')\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "cbar1.set_label('# Active Classes', rotation=270, labelpad=20)\n",
    "\n",
    "# Plot 2: Dominant class\n",
    "scatter2 = axes[0, 1].scatter(\n",
    "    tsne[:, 0], tsne[:, 1],\n",
    "    c=dominant_class,\n",
    "    cmap='tab20',\n",
    "    s=30,\n",
    "    alpha=0.7,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[0, 1].set_title('Dominant Class', fontsize=12)\n",
    "axes[0, 1].set_xlabel('t-SNE 1')\n",
    "axes[0, 1].set_ylabel('t-SNE 2')\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[0, 1], ticks=np.unique(dominant_class))\n",
    "cbar2.set_label('Class Index', rotation=270, labelpad=20)\n",
    "\n",
    "# Plot 3: Specific class membership (class 0)\n",
    "has_class_0 = train_labels_subset[:, 0]\n",
    "scatter3 = axes[1, 0].scatter(\n",
    "    tsne[:, 0], tsne[:, 1],\n",
    "    c=has_class_0,\n",
    "    cmap='RdYlBu_r',\n",
    "    s=30,\n",
    "    alpha=0.7,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[1, 0].set_title('Class 0 Membership', fontsize=12)\n",
    "axes[1, 0].set_xlabel('t-SNE 1')\n",
    "axes[1, 0].set_ylabel('t-SNE 2')\n",
    "cbar3 = plt.colorbar(scatter3, ax=axes[1, 0], ticks=[0, 1])\n",
    "cbar3.set_ticklabels(['Absent', 'Present'])\n",
    "cbar3.set_label('Has Class 0', rotation=270, labelpad=20)\n",
    "\n",
    "# Plot 4: Specific class membership (class 5)\n",
    "has_class_5 = train_labels_subset[:, 5]\n",
    "scatter4 = axes[1, 1].scatter(\n",
    "    tsne[:, 0], tsne[:, 1],\n",
    "    c=has_class_5,\n",
    "    cmap='RdYlBu_r',\n",
    "    s=30,\n",
    "    alpha=0.7,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[1, 1].set_title('Class 5 Membership', fontsize=12)\n",
    "axes[1, 1].set_xlabel('t-SNE 1')\n",
    "axes[1, 1].set_ylabel('t-SNE 2')\n",
    "cbar4 = plt.colorbar(scatter4, ax=axes[1, 1], ticks=[0, 1])\n",
    "cbar4.set_ticklabels(['Absent', 'Present'])\n",
    "cbar4.set_label('Has Class 5', rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/latent_space_tsne.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"  ✓ Saved to logs/latent_space_tsne.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# UMAP PROJECTION\n",
    "# =============================================================================\n",
    "print(\"\\nComputing UMAP projection...\")\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "umap_proj = umap_model.fit_transform(train_embeddings)\n",
    "print(f\"  ✓ UMAP shape: {umap_proj.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION: UMAP\n",
    "# =============================================================================\n",
    "print(\"\\nCreating UMAP visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('BYOL Latent Space - UMAP Projection', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Number of active classes\n",
    "scatter1 = axes[0, 0].scatter(\n",
    "    umap_proj[:, 0], umap_proj[:, 1], \n",
    "    c=label_counts, \n",
    "    cmap='viridis',\n",
    "    s=30, \n",
    "    alpha=0.7,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[0, 0].set_title('Number of Active Classes', fontsize=12)\n",
    "axes[0, 0].set_xlabel('UMAP 1')\n",
    "axes[0, 0].set_ylabel('UMAP 2')\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "cbar1.set_label('# Active Classes', rotation=270, labelpad=20)\n",
    "\n",
    "# Plot 2: Dominant class\n",
    "scatter2 = axes[0, 1].scatter(\n",
    "    umap_proj[:, 0], umap_proj[:, 1],\n",
    "    c=dominant_class,\n",
    "    cmap='tab20',\n",
    "    s=30,\n",
    "    alpha=0.7,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[0, 1].set_title('Dominant Class', fontsize=12)\n",
    "axes[0, 1].set_xlabel('UMAP 1')\n",
    "axes[0, 1].set_ylabel('UMAP 2')\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[0, 1], ticks=np.unique(dominant_class))\n",
    "cbar2.set_label('Class Index', rotation=270, labelpad=20)\n",
    "\n",
    "# Plot 3: Specific class membership (class 0)\n",
    "scatter3 = axes[1, 0].scatter(\n",
    "    umap_proj[:, 0], umap_proj[:, 1],\n",
    "    c=has_class_0,\n",
    "    cmap='RdYlBu_r',\n",
    "    s=30,\n",
    "    alpha=0.7,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[1, 0].set_title('Class 0 Membership', fontsize=12)\n",
    "axes[1, 0].set_xlabel('UMAP 1')\n",
    "axes[1, 0].set_ylabel('UMAP 2')\n",
    "cbar3 = plt.colorbar(scatter3, ax=axes[1, 0], ticks=[0, 1])\n",
    "cbar3.set_ticklabels(['Absent', 'Present'])\n",
    "cbar3.set_label('Has Class 0', rotation=270, labelpad=20)\n",
    "\n",
    "# Plot 4: Specific class membership (class 5)\n",
    "scatter4 = axes[1, 1].scatter(\n",
    "    umap_proj[:, 0], umap_proj[:, 1],\n",
    "    c=has_class_5,\n",
    "    cmap='RdYlBu_r',\n",
    "    s=30,\n",
    "    alpha=0.7,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "axes[1, 1].set_title('Class 5 Membership', fontsize=12)\n",
    "axes[1, 1].set_xlabel('UMAP 1')\n",
    "axes[1, 1].set_ylabel('UMAP 2')\n",
    "cbar4 = plt.colorbar(scatter4, ax=axes[1, 1], ticks=[0, 1])\n",
    "cbar4.set_ticklabels(['Absent', 'Present'])\n",
    "cbar4.set_label('Has Class 5', rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/latent_space_umap.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"  ✓ Saved to logs/latent_space_umap.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987ce27-97fd-417c-92f2-df32b6e20f3e",
   "metadata": {},
   "source": [
    "TODO: Save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51978741-a5c0-4910-81dd-71c6747ae4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ebe71-59cb-410a-8690-2d97727e15fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTRO-GPU (PyTorch)",
   "language": "python",
   "name": "astro-gpu-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
